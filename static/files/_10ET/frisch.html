<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Frisch-Waugh-Lovell Theorem | 10 Fundamental Theorems for Econometrics</title>
  <meta name="description" content="This book walks through the ten most important statistical theorems as highlighted by Jeffrey Wooldridge, presenting intuiitions, proofs, and applications." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Frisch-Waugh-Lovell Theorem | 10 Fundamental Theorems for Econometrics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book walks through the ten most important statistical theorems as highlighted by Jeffrey Wooldridge, presenting intuiitions, proofs, and applications." />
  <meta name="github-repo" content="tsrobinson/10EconTheorems" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Frisch-Waugh-Lovell Theorem | 10 Fundamental Theorems for Econometrics" />
  
  <meta name="twitter:description" content="This book walks through the ten most important statistical theorems as highlighted by Jeffrey Wooldridge, presenting intuiitions, proofs, and applications." />
  

<meta name="author" content="Thomas S. Robinson  (https://ts-robinson.com)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="dm.html"/>
<link rel="next" href="pd.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">10 Fundamental Theorems for Econometrics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version-notes"><i class="fa fa-check"></i>Version notes</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="exp-theorems.html"><a href="exp-theorems.html"><i class="fa fa-check"></i><b>1</b> Expectation Theorems</a><ul>
<li class="chapter" data-level="1.1" data-path="exp-theorems.html"><a href="exp-theorems.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>1.1</b> Law of Iterated Expectations</a><ul>
<li class="chapter" data-level="1.1.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-lie"><i class="fa fa-check"></i><b>1.1.1</b> Proof of LIE</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="exp-theorems.html"><a href="exp-theorems.html#law-of-total-variance"><i class="fa fa-check"></i><b>1.2</b> Law of Total Variance</a><ul>
<li class="chapter" data-level="1.2.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-ltv"><i class="fa fa-check"></i><b>1.2.1</b> Proof of LTV</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="exp-theorems.html"><a href="exp-theorems.html#linearity-of-expectations"><i class="fa fa-check"></i><b>1.3</b> Linearity of Expectations</a><ul>
<li class="chapter" data-level="1.3.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-loe"><i class="fa fa-check"></i><b>1.3.1</b> Proof of LOE</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exp-theorems.html"><a href="exp-theorems.html#variance-of-a-sum"><i class="fa fa-check"></i><b>1.4</b> Variance of a Sum</a><ul>
<li class="chapter" data-level="1.4.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-vos-x-y-are-independent"><i class="fa fa-check"></i><b>1.4.1</b> Proof of VoS: <span class="math inline">\(X, Y\)</span> are independent</a></li>
<li class="chapter" data-level="1.4.2" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-vos-x-y-are-dependent"><i class="fa fa-check"></i><b>1.4.2</b> Proof of VoS: <span class="math inline">\(X, Y\)</span> are dependent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exp-ineq.html"><a href="exp-ineq.html"><i class="fa fa-check"></i><b>2</b> Inequalities involving expectations</a><ul>
<li class="chapter" data-level="2.1" data-path="exp-ineq.html"><a href="exp-ineq.html#jensens-inequality"><i class="fa fa-check"></i><b>2.1</b> Jensen’s Inequality</a><ul>
<li class="chapter" data-level="2.1.1" data-path="exp-ineq.html"><a href="exp-ineq.html#convex-functions"><i class="fa fa-check"></i><b>2.1.1</b> Convex functions</a></li>
<li class="chapter" data-level="2.1.2" data-path="exp-ineq.html"><a href="exp-ineq.html#the-inequality"><i class="fa fa-check"></i><b>2.1.2</b> The Inequality</a></li>
<li class="chapter" data-level="2.1.3" data-path="exp-ineq.html"><a href="exp-ineq.html#proof_ji"><i class="fa fa-check"></i><b>2.1.3</b> Proof</a></li>
<li class="chapter" data-level="2.1.4" data-path="exp-ineq.html"><a href="exp-ineq.html#application"><i class="fa fa-check"></i><b>2.1.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="exp-ineq.html"><a href="exp-ineq.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>2.2</b> Chebyshev’s Inequality</a><ul>
<li class="chapter" data-level="2.2.1" data-path="exp-ineq.html"><a href="exp-ineq.html#proof"><i class="fa fa-check"></i><b>2.2.1</b> Proof</a></li>
<li class="chapter" data-level="2.2.2" data-path="exp-ineq.html"><a href="exp-ineq.html#applications"><i class="fa fa-check"></i><b>2.2.2</b> Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-projection.html"><a href="linear-projection.html"><i class="fa fa-check"></i><b>3</b> Linear Projection</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-projection.html"><a href="linear-projection.html#proj_def"><i class="fa fa-check"></i><b>3.1</b> Projection</a></li>
<li class="chapter" data-level="3.2" data-path="linear-projection.html"><a href="linear-projection.html#proj_prop"><i class="fa fa-check"></i><b>3.2</b> Properties of the projection matrix</a></li>
<li class="chapter" data-level="3.3" data-path="linear-projection.html"><a href="linear-projection.html#lp_lr"><i class="fa fa-check"></i><b>3.3</b> Linear regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-projection.html"><a href="linear-projection.html#geometric-interpretation"><i class="fa fa-check"></i><b>3.3.1</b> Geometric interpretation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="wlln.html"><a href="wlln.html"><i class="fa fa-check"></i><b>4</b> Weak Law of Large Numbers and Central Limit Theorem</a><ul>
<li class="chapter" data-level="4.1" data-path="wlln.html"><a href="wlln.html#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>4.1</b> Weak Law of Large Numbers</a><ul>
<li class="chapter" data-level="4.1.1" data-path="wlln.html"><a href="wlln.html#theorem-in-plain-english"><i class="fa fa-check"></i><b>4.1.1</b> Theorem in Plain English</a></li>
<li class="chapter" data-level="4.1.2" data-path="wlln.html"><a href="wlln.html#proof_wlln"><i class="fa fa-check"></i><b>4.1.2</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="wlln.html"><a href="wlln.html#clt"><i class="fa fa-check"></i><b>4.2</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="4.2.1" data-path="wlln.html"><a href="wlln.html#theorem-in-plain-english-1"><i class="fa fa-check"></i><b>4.2.1</b> Theorem in Plain English</a></li>
<li class="chapter" data-level="4.2.2" data-path="wlln.html"><a href="wlln.html#primer-characteristic-functions"><i class="fa fa-check"></i><b>4.2.2</b> Primer: Characteristic Functions</a></li>
<li class="chapter" data-level="4.2.3" data-path="wlln.html"><a href="wlln.html#proof-of-clt"><i class="fa fa-check"></i><b>4.2.3</b> Proof of CLT</a></li>
<li class="chapter" data-level="4.2.4" data-path="wlln.html"><a href="wlln.html#generalising-clt"><i class="fa fa-check"></i><b>4.2.4</b> Generalising CLT</a></li>
<li class="chapter" data-level="4.2.5" data-path="wlln.html"><a href="wlln.html#limitation-of-clt-and-the-importance-of-wlln"><i class="fa fa-check"></i><b>4.2.5</b> Limitation of CLT (and the importance of WLLN)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="slutsky.html"><a href="slutsky.html"><i class="fa fa-check"></i><b>5</b> Slutsky’s Theorem</a><ul>
<li class="chapter" data-level="5.1" data-path="slutsky.html"><a href="slutsky.html#theorem_slutsky"><i class="fa fa-check"></i><b>5.1</b> Theorem in plain English</a></li>
<li class="chapter" data-level="5.2" data-path="slutsky.html"><a href="slutsky.html#coded-demonstration"><i class="fa fa-check"></i><b>5.2</b> Coded demonstration</a></li>
<li class="chapter" data-level="5.3" data-path="slutsky.html"><a href="slutsky.html#proof-of-slutskys-theorem"><i class="fa fa-check"></i><b>5.3</b> Proof of Slutsky’s Theorem</a><ul>
<li class="chapter" data-level="5.3.1" data-path="slutsky.html"><a href="slutsky.html#cmt"><i class="fa fa-check"></i><b>5.3.1</b> CMT</a></li>
<li class="chapter" data-level="5.3.2" data-path="slutsky.html"><a href="slutsky.html#proof-using-cmt"><i class="fa fa-check"></i><b>5.3.2</b> Proof using CMT</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="slutsky.html"><a href="slutsky.html#applications_slutsky"><i class="fa fa-check"></i><b>5.4</b> Applications</a><ul>
<li class="chapter" data-level="5.4.1" data-path="slutsky.html"><a href="slutsky.html#proving-the-consistency-of-sample-variance-and-the-normality-of-the-t-statistic"><i class="fa fa-check"></i><b>5.4.1</b> Proving the consistency of sample variance, and the normality of the t-statistic</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html"><i class="fa fa-check"></i><b>6</b> Big Op and little op</a><ul>
<li class="chapter" data-level="6.1" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#stochastic-order-notation"><i class="fa fa-check"></i><b>6.1</b> Stochastic order notation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#relationship-of-big-o-and-little-o"><i class="fa fa-check"></i><b>6.1.1</b> Relationship of big-O and little-o</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#notational-shorthand-and-arithmetic-properties"><i class="fa fa-check"></i><b>6.2</b> Notational shorthand and ``arithmetic" properties</a></li>
<li class="chapter" data-level="6.3" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#why-is-this-usefulfn_cite1"><i class="fa fa-check"></i><b>6.3</b> Why is this useful?</a></li>
<li class="chapter" data-level="6.4" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#estimator_consistency"><i class="fa fa-check"></i><b>6.4</b> Worked Example: Consistency of mean estimators</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="dm.html"><a href="dm.html"><i class="fa fa-check"></i><b>7</b> Delta Method</a><ul>
<li class="chapter" data-level="7.1" data-path="dm.html"><a href="dm.html#delta-method-in-plain-english"><i class="fa fa-check"></i><b>7.1</b> Delta Method in Plain English</a></li>
<li class="chapter" data-level="7.2" data-path="dm.html"><a href="dm.html#proof_dm"><i class="fa fa-check"></i><b>7.2</b> Proof</a><ul>
<li class="chapter" data-level="7.2.1" data-path="dm.html"><a href="dm.html#taylors-series-and-theorem"><i class="fa fa-check"></i><b>7.2.1</b> Taylor’s Series and Theorem</a></li>
<li class="chapter" data-level="7.2.2" data-path="dm.html"><a href="dm.html#proof-of-delta-method"><i class="fa fa-check"></i><b>7.2.2</b> Proof of Delta Method</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="dm.html"><a href="dm.html#applied-example"><i class="fa fa-check"></i><b>7.3</b> Applied example</a></li>
<li class="chapter" data-level="7.4" data-path="dm.html"><a href="dm.html#alternative-strategies"><i class="fa fa-check"></i><b>7.4</b> Alternative strategies</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="frisch.html"><a href="frisch.html"><i class="fa fa-check"></i><b>8</b> Frisch-Waugh-Lovell Theorem</a><ul>
<li class="chapter" data-level="8.1" data-path="wlln.html"><a href="wlln.html#theorem-in-plain-english"><i class="fa fa-check"></i><b>8.1</b> Theorem in plain English</a></li>
<li class="chapter" data-level="8.2" data-path="frisch.html"><a href="frisch.html#proof_fw"><i class="fa fa-check"></i><b>8.2</b> Proof</a><ul>
<li class="chapter" data-level="8.2.1" data-path="frisch.html"><a href="frisch.html#primer-projection-matricessecnote"><i class="fa fa-check"></i><b>8.2.1</b> Primer: Projection matrices</a></li>
<li class="chapter" data-level="8.2.2" data-path="frisch.html"><a href="frisch.html#fwl-proof-secnote2"><i class="fa fa-check"></i><b>8.2.2</b> FWL Proof</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="frisch.html"><a href="frisch.html#coded-example"><i class="fa fa-check"></i><b>8.3</b> Coded example</a></li>
<li class="chapter" data-level="8.4" data-path="frisch.html"><a href="frisch.html#application-sensitivity-analysis"><i class="fa fa-check"></i><b>8.4</b> Application: Sensitivity analysis</a><ul>
<li class="chapter" data-level="8.4.1" data-path="frisch.html"><a href="frisch.html#regressing-the-partialled-out-x-on-the-full-y"><i class="fa fa-check"></i><b>8.4.1</b> Regressing the partialled-out X on the full Y</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="pd.html"><a href="pd.html"><i class="fa fa-check"></i><b>9</b> Positive Definite Matrices</a><ul>
<li class="chapter" data-level="9.1" data-path="pd.html"><a href="pd.html#pd_terms"><i class="fa fa-check"></i><b>9.1</b> Terminology</a><ul>
<li class="chapter" data-level="9.1.1" data-path="pd.html"><a href="pd.html#positivity"><i class="fa fa-check"></i><b>9.1.1</b> Positivity</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="pd.html"><a href="pd.html#a-b-is-psd-iff-b-1---a-1-is-psd"><i class="fa fa-check"></i><b>9.2</b> <span class="math inline">\(A-B\)</span> is PSD iff <span class="math inline">\(B^{-1} - A^{-1}\)</span> is PSD</a><ul>
<li class="chapter" data-level="9.2.1" data-path="pd.html"><a href="pd.html#pd_proof"><i class="fa fa-check"></i><b>9.2.1</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="pd.html"><a href="pd.html#pd_app"><i class="fa fa-check"></i><b>9.3</b> Applications</a><ul>
<li class="chapter" data-level="9.3.1" data-path="pd.html"><a href="pd.html#ols-as-the-best-linear-unbiased-estimator-blue"><i class="fa fa-check"></i><b>9.3.1</b> OLS as the best linear unbiased estimator (BLUE)</a></li>
<li class="chapter" data-level="9.3.2" data-path="pd.html"><a href="pd.html#optimisation-problems"><i class="fa fa-check"></i><b>9.3.2</b> Optimisation problems</a></li>
<li class="chapter" data-level="9.3.3" data-path="pd.html"><a href="pd.html#recap"><i class="fa fa-check"></i><b>9.3.3</b> Recap</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">10 Fundamental Theorems for Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="frisch" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Frisch-Waugh-Lovell Theorem</h1>
<div id="theorem-in-plain-english" class="section level2">
<h2><span class="header-section-number">8.1</span> Theorem in plain English</h2>
<p>The Frisch-Waugh-Lovell Theorem (FWL; after the initial proof by <span class="citation">Frisch and Waugh (<a href="#ref-frisch1933partial" role="doc-biblioref">1933</a>)</span>, and later generalisation by <span class="citation">Lovell (<a href="#ref-lovell1963seasonal" role="doc-biblioref">1963</a>)</span>) states that:</p>
<p>Any predictor’s regression coefficient in a multivariate model is equivalent to the regression coefficient estimated from a bivariate model in which the residualised outcome is regressed on the residualised component of the predictor; where the residuals are taken from models regressing the outcome <em>and</em> the predictor on all other predictors in the multivariate regression (separately).</p>
<p>More formally, assume we have a multivariate regression model with <span class="math inline">\(k\)</span> predictors:</p>
<p><span class="math display">\[\begin{equation}
\hat{y} = \hat{\beta}_{1}x_{1} + ... \hat{\beta}_{k}x_{k} + \epsilon. \label{eq:multi}
\end{equation}\]</span></p>
<p>FWL states that every <span class="math inline">\(\hat{\beta}_{j}\)</span> in Equation  is equal to <span class="math inline">\(\hat{\beta}^{*}_{j}\)</span>,  the residual <span class="math inline">\(\epsilon = \epsilon^{*}\)</span> in:</p>
<p><span class="math display">\[\begin{equation}
     \epsilon^{y} = \hat{\beta}^{*}_{j}\epsilon^{x_j} + \epsilon^{*} 
\end{equation}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
\begin{aligned}
     \epsilon^y &amp; = y - \sum_{k \neq j}\hat{\beta}^{y}_{k}x_{k} \\
     \epsilon^{x_{j}} &amp;= x_j - \sum_{k \neq j}\hat{\beta}^{x_{j}}_{k}x_{k}.
\end{aligned}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\hat{\beta}^{y}_k\)</span> and <span class="math inline">\(\hat{\beta}^{x_j}_k\)</span> are the regression coefficients from two separate regression models of the outcome (omitting <span class="math inline">\(x_j\)</span>) and <span class="math inline">\(x_j\)</span> respectively.</p>
<p>In other words, FWL states that each predictor’s coefficient in a multivariate regression explains that variance of <span class="math inline">\(y\)</span> not explained by both the other k-1 predictors’ relationship with the outcome and their relationship with that predictor, i.e. the independent effect of <span class="math inline">\(x_j\)</span>.</p>
</div>
<div id="proof_fw" class="section level2">
<h2><span class="header-section-number">8.2</span> Proof</h2>
<div id="primer-projection-matricessecnote" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Primer: Projection matrices<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></h3>
<p>We need two important types of projection matrices to understand the linear algebra proof of FWL. First, the <strong>prediction matrix</strong> that was introduced in <a href="linear-projection.html#linear_projection">Chapter 4</a>:</p>
<p><span class="math display">\[\begin{equation}
    P = X(X&#39;X)^{-1}X&#39;.
\end{equation}\]</span></p>
<p>Recall that this matrix, when applied to an outcome vector (<span class="math inline">\(y\)</span>), produces a set of predicted values (<span class="math inline">\(\hat{y}\)</span>). Reverse engineering this, note that <span class="math inline">\(\hat{y}=X\hat{\beta}=X(X&#39;X)^{-1}X&#39;y = Py\)</span>.</p>
<p>Since <span class="math inline">\(Py\)</span> produces the predicted values from a regression on <span class="math inline">\(X\)</span>, we can define its complement, the :</p>
<p><span class="math display">\[\begin{equation}
    M = I - X(X&#39;X)^{-1}X&#39;,
\end{equation}\]</span></p>
<p>since <span class="math inline">\(My = y - X(X&#39;X)^{-1}X&#39;y \equiv y-Py \equiv y - X\hat{\beta} \equiv \hat{\epsilon},\)</span> the residuals from regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>.</p>
<p>Given these definitions, note that M and P are complementary:</p>
<p><span class="math display">\[\begin{align}
\begin{aligned}
    y  &amp;= \hat{y} + \hat{\epsilon} \\
       &amp;\equiv Py + My \\ 
    Iy &amp;= Py + My \\
    Iy &amp;= (P + M)y \\
    I  &amp;= P + M.
\end{aligned}
\end{align}\]</span></p>
<p>With these projection matrices, we can express the FWL claim (which we need to prove) as:</p>
<p><span class="math display">\[\begin{align}
\begin{aligned}
    y &amp;= X_{1}\hat{\beta_{1}} + X_{2}\hat{\beta_{2}} + \hat{\epsilon} \\
    M_{1}y &amp;= M_{1}X_2\hat{\beta}_{2} + \hat{\epsilon}, \label{projection_statement}
\end{aligned}
\end{align}\]</span></p>
</div>
<div id="fwl-proof-secnote2" class="section level3">
<h3><span class="header-section-number">8.2.2</span> FWL Proof<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></h3>
<p>Let us assume, as in Equation  that:</p>
<p><span class="math display">\[\begin{equation}
    Y = X_1\hat{\beta}_1 + X_2\hat{\beta}_2 + \hat{\epsilon}. \label{eq:multivar}
\end{equation}\]</span></p>
<p>First, we can multiply both sides by the residual maker of <span class="math inline">\(X_1\)</span>:</p>
<p><span class="math display">\[\begin{equation}
    M_1Y = M_1X_1\hat{\beta}_1 + M_1X_2\hat{\beta}_2 + M_1\hat{\epsilon},
\end{equation}\]</span></p>
<p>which first simplifies to:</p>
<p><span class="math display">\[\begin{equation}
    M_1Y = M_1X_2\hat{\beta}_2 + M_1\hat{\epsilon}, \label{eq:part_simp}
\end{equation}\]</span></p>
<p>because <span class="math inline">\(M_1X_1\hat{\beta}_1 \equiv (M_1X_1)\hat{\beta}_1 \equiv 0\hat{\beta}_1 = 0\)</span>. In plain English, by definition, all the variance in <span class="math inline">\(X_1\)</span> is explained by <span class="math inline">\(X_1\)</span> and therefore a regression of <span class="math inline">\(X_1\)</span> on itself leaves no part unexplained so <span class="math inline">\(M_1X_1\)</span> is zero.</p>
<p>Second, we can simplify this equation further because, by the properties of OLS regression, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(\epsilon\)</span> are orthogonal. Therefore the residual of the residuals are the residuals! Hence:</p>
<p><span class="math display">\[\begin{equation*}
    M_1Y = M_1X_2\hat{\beta}_2 + \hat{\epsilon} \; \; \; \square.
\end{equation*}\]</span></p>

<p>A couple of interesting features come out of the linear algebra proof:</p>
<ul>
<li><p>FWL also holds for bivariate regression when you first residualise Y and X on a <span class="math inline">\(n\times1\)</span> vector of 1’s (i.e. the constant) – which is like demeaning the outcome and predictor before regressing the two.</p></li>
<li><p><span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are technically  of mutually exclusive predictors i.e. <span class="math inline">\(X_1\)</span> is an <span class="math inline">\(n \times k\)</span> matrix <span class="math inline">\(\{X_1,...,X_k\}\)</span>, and <span class="math inline">\(X_2\)</span> is an <span class="math inline">\(n \times m\)</span> matrix <span class="math inline">\(\{X_{k+1},...,X_{k+m}\}\)</span>, where <span class="math inline">\(\beta_1\)</span> is a corresponding vector of regression coefficients <span class="math inline">\(\beta_1 = \{\gamma_1,...,\gamma_k\}\)</span>, and likewise <span class="math inline">\(\beta_2 = \{\delta_1,..., \delta_m\}\)</span>, such that:</p>
<p><span class="math display">\[\begin{align*}
  Y &amp;= X_1\beta_1 + X_2\beta_2 \\
    &amp;= X_{1}\hat{\gamma}_1 + ... + X_{k}\hat{\gamma}_k + X_{k+1}\hat{\delta}_{1} + ... + X_{k+m}\hat{\delta}_{m},
  \end{align*}\]</span></p></li>
</ul>
<p>Hence the FWL theorem is exceptionally general, applying not only to arbitrarily long coefficient vectors, but also enabling you to back out estimates from any partitioning of the full regression model.</p>
</div>
</div>
<div id="coded-example" class="section level2">
<h2><span class="header-section-number">8.3</span> Coded example</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="frisch.html#cb6-1"></a><span class="kw">set.seed</span>(<span class="dv">89</span>)</span>
<span id="cb6-2"><a href="frisch.html#cb6-2"></a></span>
<span id="cb6-3"><a href="frisch.html#cb6-3"></a><span class="co">## Generate random data</span></span>
<span id="cb6-4"><a href="frisch.html#cb6-4"></a>df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> <span class="kw">rnorm</span>(<span class="dv">1000</span>,<span class="dv">2</span>,<span class="fl">1.5</span>),</span>
<span id="cb6-5"><a href="frisch.html#cb6-5"></a>                 <span class="dt">x1 =</span> <span class="kw">rnorm</span>(<span class="dv">1000</span>,<span class="dv">1</span>,<span class="fl">0.3</span>),</span>
<span id="cb6-6"><a href="frisch.html#cb6-6"></a>                 <span class="dt">x2 =</span> <span class="kw">rnorm</span>(<span class="dv">1000</span>,<span class="dv">1</span>,<span class="dv">4</span>))</span>
<span id="cb6-7"><a href="frisch.html#cb6-7"></a></span>
<span id="cb6-8"><a href="frisch.html#cb6-8"></a><span class="co">## Partial regressions</span></span>
<span id="cb6-9"><a href="frisch.html#cb6-9"></a></span>
<span id="cb6-10"><a href="frisch.html#cb6-10"></a><span class="co"># Residual of y regressed on x1</span></span>
<span id="cb6-11"><a href="frisch.html#cb6-11"></a>y_res &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1, df)<span class="op">$</span>residuals</span>
<span id="cb6-12"><a href="frisch.html#cb6-12"></a></span>
<span id="cb6-13"><a href="frisch.html#cb6-13"></a><span class="co"># Residual of x2 regressed on x1</span></span>
<span id="cb6-14"><a href="frisch.html#cb6-14"></a>x_res &lt;-<span class="st"> </span><span class="kw">lm</span>(x2 <span class="op">~</span><span class="st"> </span>x1, df)<span class="op">$</span>residuals</span>
<span id="cb6-15"><a href="frisch.html#cb6-15"></a></span>
<span id="cb6-16"><a href="frisch.html#cb6-16"></a>resids &lt;-<span class="st"> </span><span class="kw">data.frame</span>(y_res, x_res)</span>
<span id="cb6-17"><a href="frisch.html#cb6-17"></a></span>
<span id="cb6-18"><a href="frisch.html#cb6-18"></a><span class="co">## Compare the beta values for x2</span></span>
<span id="cb6-19"><a href="frisch.html#cb6-19"></a><span class="co"># Multivariate regression:</span></span>
<span id="cb6-20"><a href="frisch.html#cb6-20"></a><span class="kw">summary</span>(<span class="kw">lm</span>(y<span class="op">~</span>x1<span class="op">+</span>x2, df))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.451 -1.001 -0.039  1.072  5.320 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.33629    0.16427  14.222   &lt;2e-16 ***
## x1          -0.31093    0.15933  -1.952   0.0513 .  
## x2           0.02023    0.01270   1.593   0.1116    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.535 on 997 degrees of freedom
## Multiple R-squared:  0.006252,	Adjusted R-squared:  0.004258 
## F-statistic: 3.136 on 2 and 997 DF,  p-value: 0.04388</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="frisch.html#cb8-1"></a><span class="co"># Partials regression</span></span>
<span id="cb8-2"><a href="frisch.html#cb8-2"></a><span class="kw">summary</span>(<span class="kw">lm</span>(y_res <span class="op">~</span><span class="st"> </span>x_res, resids))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y_res ~ x_res, data = resids)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.451 -1.001 -0.039  1.072  5.320 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -8.921e-17  4.850e-02   0.000    1.000
## x_res        2.023e-02  1.270e-02   1.593    0.111
## 
## Residual standard error: 1.534 on 998 degrees of freedom
## Multiple R-squared:  0.002538,	Adjusted R-squared:  0.001538 
## F-statistic: 2.539 on 1 and 998 DF,  p-value: 0.1114</code></pre>
<p><strong>Note:</strong> This isn’t an exact demonstration because there is a degrees of freedom of error. The (correct) multivariate regression degrees of freedom is calculated as <span class="math inline">\(N - 3\)</span> since there are three variables. In the partial regression the degrees of freedom is <span class="math inline">\(N - 2\)</span>. This latter calculation does not take into account the additional loss of freedom as a result of partialling out <span class="math inline">\(X_1\)</span>.</p>
</div>
<div id="application-sensitivity-analysis" class="section level2">
<h2><span class="header-section-number">8.4</span> Application: Sensitivity analysis</h2>
<p><span class="citation">Cinelli and Hazlett (<a href="#ref-cinelli2020making" role="doc-biblioref">2020</a>)</span> develop a series of tools for researchers to conduct sensitivity analyses on regression models, using an extension of the omitted variable bias framework. To do so, they use FWL to motivate this bias. Suppose that the full regression model is specified as:</p>
<p><span class="math display">\[\begin{equation}
    Y = \hat{\tau}D + X\hat{\beta} + \hat{\gamma}Z + \hat{\epsilon}_{\text{full}}, \label{eq:ch_full}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat{\tau}, \hat{\beta}, \hat{\gamma}\)</span> are estimated regression coefficients, D is the treatment variable, X are observed covariates, and Z are unobserved covariates. Since, Z is unobserved, researchers measure:</p>
<p><span class="math display">\[\begin{equation}
Y = \hat{\tau}_\text{Obs.}D + X\hat{\beta}_\text{Obs.} + \epsilon_\text{Obs}
\end{equation}\]</span></p>
<p>By FWL, we know that <span class="math inline">\(\hat{\tau}_\text{Obs.}\)</span> is equivalent to the coefficient of regressing the residualised outcome (with respect to X), on the residualised outcome of D (again with respect to X). Call these two residuals <span class="math inline">\(Y_r\)</span> and <span class="math inline">\(D_r\)</span>.</p>
<p>And recall that the regression model for the final-stage of the partial regressions is bivariate (<span class="math inline">\(Y_r \sim D_r\)</span>). Conveniently, a bivariate regression coefficient can be expressed in terms of the covariance between the left-hand and right-hand side variables:</p>
<p><span class="math display">\[\begin{equation}
    \hat{\tau}_\text{Obs.} = \frac{cov(D_r, Y_r)}{var(D_r)}.
\end{equation}\]</span></p>
<p>Note that given the full regression model in Equation , the partial outcome <span class="math inline">\(Y_r\)</span> is actually composed of the elements <span class="math inline">\(\hat{\tau}D_r + \hat{\gamma}Z_r\)</span>, and so:</p>
<p><span class="math display">\[\begin{equation}
    \hat{\tau}_\text{Obs.} = \frac{cov(D_r, \hat{\tau}D_r + \hat{\gamma}Z_r)}{var(D_r)} \label{eq:cov_expand}
\end{equation}\]</span></p>
<p>Next, we can expand the covariance using the expectation rule that <span class="math inline">\(cov(A, B+C) = cov(A,B) + cov(A,C)\)</span> and since <span class="math inline">\(\hat{\tau},\hat{\gamma}\)</span> are scalar, we can move them outside the covariance functions:</p>
<p><span class="math display">\[\begin{equation}
    \hat{\tau}_\text{Obs.} = \frac{\hat{\tau}cov(D_r, D_r) + \hat{\gamma}cov(D_r,Z_r)}{var(D_r)}
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(cov(A,A) = var(A)\)</span> and therefore:</p>
<p><span class="math display">\[\begin{equation}
    \hat{\tau}_\text{Obs.} = \frac{\hat{\tau}var(D_r) + \hat{\gamma}cov(D_r,Z_r)}{var(D_r)} \equiv \hat{\tau} + \hat{\gamma}\frac{cov(D_r,Z_r)}{var(D_r)} \equiv \hat{\tau} + \hat{\gamma}\hat{\delta}
\end{equation}\]</span></p>
<p>Frisch-Waugh is so useful because it simplifies a multivariate equation into a bivariate one. While computationally this makes zero difference (unlike in the days of hand computation), here it allows us to use a convenient expression of the bivariate coefficient to show and quantify the bias when you run a regression in the presence of an unobserved confounder. Moreover, note that in Equation , we implicitly use FWL again since we know that the non-stochastic aspect of Y not explained by X are the residualised components of the full Equation .</p>
<div id="regressing-the-partialled-out-x-on-the-full-y" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Regressing the partialled-out X on the full Y</h3>
<p>In <em>Mostly Harmless Econometrics</em> (MHE; <span class="citation">Angrist and Pischke (<a href="#ref-AngristMostlyHarmlessEconometrics2009" role="doc-biblioref">2009</a>)</span>), the authors note that you also get an identical coefficient to the full regression if you regress the residualised predictor on the non-residualised <span class="math inline">\(Y\)</span>. We can use the OVB framework above to explain this case.</p>
<p>Let’s take the full regression model as:</p>
<p><span class="math display">\[\begin{equation}
Y = \hat{\beta}_1X_1 + \hat{\beta}_2X_2 + \hat{\epsilon}.    
\end{equation}\]</span></p>
<p> states that:</p>
<p><span class="math display">\[\begin{equation}
    Y = \hat{\beta_1}M_2X_1 + \hat{\epsilon}.
\end{equation}\]</span></p>
<p>Note that this is just FWL, except we have not also residualised <span class="math inline">\(Y\)</span>. Our aim is to check whether there is any bias in the estimated coefficient from this second equation. As before, since this is a bivariate regression we can express the coefficient as:</p>
<p><span class="math display">\[\begin{align}
    \begin{aligned}
    \hat{\beta}_1 &amp;= \frac{cov(M_2X_1, Y)}{var(M_2X_1)} \\
    &amp;= \frac{cov(M_2X_1, \hat{\beta}_1X_1 + \hat{\beta}_2X_2)}{var(M_2X_1)} \\
    &amp;= \hat{\beta}_1\frac{cov(M_2X_1,X_1)}{var(M_2X_1)} + \hat{\beta}_2\frac{cov(M_2X_1,X_2)}{var(M_2X_1)} \\
    &amp;= \hat{\beta}_1 + \hat{\beta}_2\times 0 \\
    &amp;= \hat{\beta}_1
    \end{aligned}
\end{align}\]</span></p>
<p>This follows from two features. First, <span class="math inline">\(cov(M_2X_1,X_1) = var(M_2X_1)\)</span>. Second, it is clear that <span class="math inline">\(cov(M_2X_1, X_2) =0\)</span> because <span class="math inline">\(M_2X_1\)</span> is <span class="math inline">\(X_1\)</span> stripped of any variance associated with <span class="math inline">\(X_2\)</span> and so, by definition, they do not covary. Therefore, we can recover the unbiased regression coefficient using an adapted version of FWL where we do not residualise Y – as stated in <em>MHE</em>.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-AngristMostlyHarmlessEconometrics2009">
<p>Angrist, Joshua D., and Jörn-Steffen Pischke. 2009. <em>Mostly Harmless Econometrics: An Empiricist’s Companion</em>. Princeton: Princeton University Press.</p>
</div>
<div id="ref-cinelli2020making">
<p>Cinelli, Carlos, and Chad Hazlett. 2020. “Making Sense of Sensitivity: Extending Omitted Variable Bias.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 82 (1): 39–67.</p>
</div>
<div id="ref-frisch1933partial">
<p>Frisch, Ragnar, and Frederick V Waugh. 1933. “Partial Time Regressions as Compared with Individual Trends.” <em>Econometrica: Journal of the Econometric Society</em>, 387–401.</p>
</div>
<div id="ref-lovell1963seasonal">
<p>Lovell, Michael C. 1963. “Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis.” <em>Journal of the American Statistical Association</em> 58 (304): 993–1010.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p><em>Citation</em> Based on <a href="https://www.uio.no/studier/emner/sv/oekonomi/ECON4160/h13/undervisningsmateriale/lnote1h13-.pdf">lecture notes</a> from the University of Oslo’s ``Econometrics – Modelling and Systems Estimation" course (author attribution unclear), and <span class="citation">Davidson, MacKinnon, and others (<a href="#ref-davidson2004econometric" role="doc-biblioref">2004</a>)</span>.}<a href="frisch.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p><em>Citation</em>: Adapted from York University, Canada’s <a href="http://scs.math.yorku.ca/index.php/Statistics:_Frisch-Waugh-Lovell_and_GLS/Proof_of_the_FWL_for_OLS">wiki for statistical consulting</a>.<a href="frisch.html#fnref11" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="dm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pd.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09-frisch-waugh.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["10EconometricTheorems.pdf", "10EconometricTheorems.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
