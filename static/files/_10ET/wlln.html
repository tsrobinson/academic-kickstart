<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Weak Law of Large Numbers and Central Limit Theorem | 10 Fundamental Theorems for Econometrics</title>
  <meta name="description" content="This book walks through the ten most important statistical theorems as highlighted by Jeffrey Wooldridge, presenting intuiitions, proofs, and applications." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Weak Law of Large Numbers and Central Limit Theorem | 10 Fundamental Theorems for Econometrics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book walks through the ten most important statistical theorems as highlighted by Jeffrey Wooldridge, presenting intuiitions, proofs, and applications." />
  <meta name="github-repo" content="tsrobinson/10EconTheorems" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Weak Law of Large Numbers and Central Limit Theorem | 10 Fundamental Theorems for Econometrics" />
  
  <meta name="twitter:description" content="This book walks through the ten most important statistical theorems as highlighted by Jeffrey Wooldridge, presenting intuiitions, proofs, and applications." />
  

<meta name="author" content="Thomas S. Robinson  (https://ts-robinson.com)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-projection.html"/>
<link rel="next" href="slutsky.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">10 Fundamental Theorems for Econometrics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version-notes"><i class="fa fa-check"></i>Version notes</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="exp-theorems.html"><a href="exp-theorems.html"><i class="fa fa-check"></i><b>1</b> Expectation Theorems</a><ul>
<li class="chapter" data-level="1.1" data-path="exp-theorems.html"><a href="exp-theorems.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>1.1</b> Law of Iterated Expectations</a><ul>
<li class="chapter" data-level="1.1.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-lie"><i class="fa fa-check"></i><b>1.1.1</b> Proof of LIE</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="exp-theorems.html"><a href="exp-theorems.html#law-of-total-variance"><i class="fa fa-check"></i><b>1.2</b> Law of Total Variance</a><ul>
<li class="chapter" data-level="1.2.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-ltv"><i class="fa fa-check"></i><b>1.2.1</b> Proof of LTV</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="exp-theorems.html"><a href="exp-theorems.html#linearity-of-expectations"><i class="fa fa-check"></i><b>1.3</b> Linearity of Expectations</a><ul>
<li class="chapter" data-level="1.3.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-loe"><i class="fa fa-check"></i><b>1.3.1</b> Proof of LOE</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exp-theorems.html"><a href="exp-theorems.html#variance-of-a-sum"><i class="fa fa-check"></i><b>1.4</b> Variance of a Sum</a><ul>
<li class="chapter" data-level="1.4.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-vos-x-y-are-independent"><i class="fa fa-check"></i><b>1.4.1</b> Proof of VoS: <span class="math inline">\(X, Y\)</span> are independent</a></li>
<li class="chapter" data-level="1.4.2" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-vos-x-y-are-dependent"><i class="fa fa-check"></i><b>1.4.2</b> Proof of VoS: <span class="math inline">\(X, Y\)</span> are dependent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exp-ineq.html"><a href="exp-ineq.html"><i class="fa fa-check"></i><b>2</b> Inequalities involving expectations</a><ul>
<li class="chapter" data-level="2.1" data-path="exp-ineq.html"><a href="exp-ineq.html#jensens-inequality"><i class="fa fa-check"></i><b>2.1</b> Jensen’s Inequality</a><ul>
<li class="chapter" data-level="2.1.1" data-path="exp-ineq.html"><a href="exp-ineq.html#convex-functions"><i class="fa fa-check"></i><b>2.1.1</b> Convex functions</a></li>
<li class="chapter" data-level="2.1.2" data-path="exp-ineq.html"><a href="exp-ineq.html#the-inequality"><i class="fa fa-check"></i><b>2.1.2</b> The Inequality</a></li>
<li class="chapter" data-level="2.1.3" data-path="exp-ineq.html"><a href="exp-ineq.html#proof_ji"><i class="fa fa-check"></i><b>2.1.3</b> Proof</a></li>
<li class="chapter" data-level="2.1.4" data-path="exp-ineq.html"><a href="exp-ineq.html#application"><i class="fa fa-check"></i><b>2.1.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="exp-ineq.html"><a href="exp-ineq.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>2.2</b> Chebyshev’s Inequality</a><ul>
<li class="chapter" data-level="2.2.1" data-path="exp-ineq.html"><a href="exp-ineq.html#proof"><i class="fa fa-check"></i><b>2.2.1</b> Proof</a></li>
<li class="chapter" data-level="2.2.2" data-path="exp-ineq.html"><a href="exp-ineq.html#applications"><i class="fa fa-check"></i><b>2.2.2</b> Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-projection.html"><a href="linear-projection.html"><i class="fa fa-check"></i><b>3</b> Linear Projection</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-projection.html"><a href="linear-projection.html#proj_def"><i class="fa fa-check"></i><b>3.1</b> Projection</a></li>
<li class="chapter" data-level="3.2" data-path="linear-projection.html"><a href="linear-projection.html#proj_prop"><i class="fa fa-check"></i><b>3.2</b> Properties of the projection matrix</a></li>
<li class="chapter" data-level="3.3" data-path="linear-projection.html"><a href="linear-projection.html#lp_lr"><i class="fa fa-check"></i><b>3.3</b> Linear regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-projection.html"><a href="linear-projection.html#geometric-interpretation"><i class="fa fa-check"></i><b>3.3.1</b> Geometric interpretation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="wlln.html"><a href="wlln.html"><i class="fa fa-check"></i><b>4</b> Weak Law of Large Numbers and Central Limit Theorem</a><ul>
<li class="chapter" data-level="4.1" data-path="wlln.html"><a href="wlln.html#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>4.1</b> Weak Law of Large Numbers</a><ul>
<li class="chapter" data-level="4.1.1" data-path="wlln.html"><a href="wlln.html#theorem-in-plain-english"><i class="fa fa-check"></i><b>4.1.1</b> Theorem in Plain English</a></li>
<li class="chapter" data-level="4.1.2" data-path="wlln.html"><a href="wlln.html#proof_wlln"><i class="fa fa-check"></i><b>4.1.2</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="wlln.html"><a href="wlln.html#clt"><i class="fa fa-check"></i><b>4.2</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="4.2.1" data-path="wlln.html"><a href="wlln.html#theorem-in-plain-english-1"><i class="fa fa-check"></i><b>4.2.1</b> Theorem in Plain English</a></li>
<li class="chapter" data-level="4.2.2" data-path="wlln.html"><a href="wlln.html#primer-characteristic-functions"><i class="fa fa-check"></i><b>4.2.2</b> Primer: Characteristic Functions</a></li>
<li class="chapter" data-level="4.2.3" data-path="wlln.html"><a href="wlln.html#proof-of-clt"><i class="fa fa-check"></i><b>4.2.3</b> Proof of CLT</a></li>
<li class="chapter" data-level="4.2.4" data-path="wlln.html"><a href="wlln.html#generalising-clt"><i class="fa fa-check"></i><b>4.2.4</b> Generalising CLT</a></li>
<li class="chapter" data-level="4.2.5" data-path="wlln.html"><a href="wlln.html#limitation-of-clt-and-the-importance-of-wlln"><i class="fa fa-check"></i><b>4.2.5</b> Limitation of CLT (and the importance of WLLN)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="slutsky.html"><a href="slutsky.html"><i class="fa fa-check"></i><b>5</b> Slutsky’s Theorem</a><ul>
<li class="chapter" data-level="5.1" data-path="slutsky.html"><a href="slutsky.html#theorem_slutsky"><i class="fa fa-check"></i><b>5.1</b> Theorem in plain English</a></li>
<li class="chapter" data-level="5.2" data-path="slutsky.html"><a href="slutsky.html#coded-demonstration"><i class="fa fa-check"></i><b>5.2</b> Coded demonstration</a></li>
<li class="chapter" data-level="5.3" data-path="slutsky.html"><a href="slutsky.html#proof-of-slutskys-theorem"><i class="fa fa-check"></i><b>5.3</b> Proof of Slutsky’s Theorem</a><ul>
<li class="chapter" data-level="5.3.1" data-path="slutsky.html"><a href="slutsky.html#cmt"><i class="fa fa-check"></i><b>5.3.1</b> CMT</a></li>
<li class="chapter" data-level="5.3.2" data-path="slutsky.html"><a href="slutsky.html#proof-using-cmt"><i class="fa fa-check"></i><b>5.3.2</b> Proof using CMT</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="slutsky.html"><a href="slutsky.html#applications_slutsky"><i class="fa fa-check"></i><b>5.4</b> Applications</a><ul>
<li class="chapter" data-level="5.4.1" data-path="slutsky.html"><a href="slutsky.html#proving-the-consistency-of-sample-variance-and-the-normality-of-the-t-statistic"><i class="fa fa-check"></i><b>5.4.1</b> Proving the consistency of sample variance, and the normality of the t-statistic</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html"><i class="fa fa-check"></i><b>6</b> Big Op and little op</a><ul>
<li class="chapter" data-level="6.1" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#stochastic-order-notation"><i class="fa fa-check"></i><b>6.1</b> Stochastic order notation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#relationship-of-big-o-and-little-o"><i class="fa fa-check"></i><b>6.1.1</b> Relationship of big-O and little-o</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#notational-shorthand-and-arithmetic-properties"><i class="fa fa-check"></i><b>6.2</b> Notational shorthand and ``arithmetic" properties</a></li>
<li class="chapter" data-level="6.3" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#why-is-this-usefulfn_cite1"><i class="fa fa-check"></i><b>6.3</b> Why is this useful?</a></li>
<li class="chapter" data-level="6.4" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#estimator_consistency"><i class="fa fa-check"></i><b>6.4</b> Worked Example: Consistency of mean estimators</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="dm.html"><a href="dm.html"><i class="fa fa-check"></i><b>7</b> Delta Method</a><ul>
<li class="chapter" data-level="7.1" data-path="dm.html"><a href="dm.html#delta-method-in-plain-english"><i class="fa fa-check"></i><b>7.1</b> Delta Method in Plain English</a></li>
<li class="chapter" data-level="7.2" data-path="dm.html"><a href="dm.html#proof_dm"><i class="fa fa-check"></i><b>7.2</b> Proof</a><ul>
<li class="chapter" data-level="7.2.1" data-path="dm.html"><a href="dm.html#taylors-series-and-theorem"><i class="fa fa-check"></i><b>7.2.1</b> Taylor’s Series and Theorem</a></li>
<li class="chapter" data-level="7.2.2" data-path="dm.html"><a href="dm.html#proof-of-delta-method"><i class="fa fa-check"></i><b>7.2.2</b> Proof of Delta Method</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="dm.html"><a href="dm.html#applied-example"><i class="fa fa-check"></i><b>7.3</b> Applied example</a></li>
<li class="chapter" data-level="7.4" data-path="dm.html"><a href="dm.html#alternative-strategies"><i class="fa fa-check"></i><b>7.4</b> Alternative strategies</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="frisch.html"><a href="frisch.html"><i class="fa fa-check"></i><b>8</b> Frisch-Waugh-Lovell Theorem</a><ul>
<li class="chapter" data-level="8.1" data-path="wlln.html"><a href="wlln.html#theorem-in-plain-english"><i class="fa fa-check"></i><b>8.1</b> Theorem in plain English</a></li>
<li class="chapter" data-level="8.2" data-path="frisch.html"><a href="frisch.html#proof_fw"><i class="fa fa-check"></i><b>8.2</b> Proof</a><ul>
<li class="chapter" data-level="8.2.1" data-path="frisch.html"><a href="frisch.html#primer-projection-matricessecnote"><i class="fa fa-check"></i><b>8.2.1</b> Primer: Projection matrices</a></li>
<li class="chapter" data-level="8.2.2" data-path="frisch.html"><a href="frisch.html#fwl-proof-secnote2"><i class="fa fa-check"></i><b>8.2.2</b> FWL Proof</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="frisch.html"><a href="frisch.html#coded-example"><i class="fa fa-check"></i><b>8.3</b> Coded example</a></li>
<li class="chapter" data-level="8.4" data-path="frisch.html"><a href="frisch.html#application-sensitivity-analysis"><i class="fa fa-check"></i><b>8.4</b> Application: Sensitivity analysis</a><ul>
<li class="chapter" data-level="8.4.1" data-path="frisch.html"><a href="frisch.html#regressing-the-partialled-out-x-on-the-full-y"><i class="fa fa-check"></i><b>8.4.1</b> Regressing the partialled-out X on the full Y</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="pd.html"><a href="pd.html"><i class="fa fa-check"></i><b>9</b> Positive Definite Matrices</a><ul>
<li class="chapter" data-level="9.1" data-path="pd.html"><a href="pd.html#pd_terms"><i class="fa fa-check"></i><b>9.1</b> Terminology</a><ul>
<li class="chapter" data-level="9.1.1" data-path="pd.html"><a href="pd.html#positivity"><i class="fa fa-check"></i><b>9.1.1</b> Positivity</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="pd.html"><a href="pd.html#a-b-is-psd-iff-b-1---a-1-is-psd"><i class="fa fa-check"></i><b>9.2</b> <span class="math inline">\(A-B\)</span> is PSD iff <span class="math inline">\(B^{-1} - A^{-1}\)</span> is PSD</a><ul>
<li class="chapter" data-level="9.2.1" data-path="pd.html"><a href="pd.html#pd_proof"><i class="fa fa-check"></i><b>9.2.1</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="pd.html"><a href="pd.html#pd_app"><i class="fa fa-check"></i><b>9.3</b> Applications</a><ul>
<li class="chapter" data-level="9.3.1" data-path="pd.html"><a href="pd.html#ols-as-the-best-linear-unbiased-estimator-blue"><i class="fa fa-check"></i><b>9.3.1</b> OLS as the best linear unbiased estimator (BLUE)</a></li>
<li class="chapter" data-level="9.3.2" data-path="pd.html"><a href="pd.html#optimisation-problems"><i class="fa fa-check"></i><b>9.3.2</b> Optimisation problems</a></li>
<li class="chapter" data-level="9.3.3" data-path="pd.html"><a href="pd.html#recap"><i class="fa fa-check"></i><b>9.3.3</b> Recap</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">10 Fundamental Theorems for Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="wlln" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Weak Law of Large Numbers and Central Limit Theorem</h1>
<p>This chapter focuses on two fundamental theorems that form the basis of our inferences from samples to populations. The Weak Law of Large Numbers (WLLN) provides the basis for generalisation from a sample mean to the population mean. The Central Limit Theorem (CLT) provides the basis for quantifying our uncertainty over this parameter. In both cases, I discuss the theorem itself and provide an annotated proof. Finally, I discuss how the two theorems complement each other.</p>
<div id="weak-law-of-large-numbers" class="section level2">
<h2><span class="header-section-number">4.1</span> Weak Law of Large Numbers</h2>
<div id="theorem-in-plain-english" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Theorem in Plain English</h3>
<p>Suppose we have a random variable <span class="math inline">\(X\)</span>. From <span class="math inline">\(X\)</span>, we can generate a sequence of random variables <span class="math inline">\(X_1, X_2,..., X_n\)</span> that are independent and identically distributed (i.i.d.) draws of <span class="math inline">\(X\)</span>. Assuming <span class="math inline">\(n\)</span> is finite, we can perform calculations on this sequence of random numbers. For example, we can calculate the mean of the sequence <span class="math inline">\(\bar{X}_n = \frac{1}{n}\sum^n_{i=1}X_i\)</span>. This value is the sample mean – from a much wider population, we have drawn a finite sequence of observations, and calculated the average across them. How do we know that this sample parameter is meaningful with respect to the population, and therefore that we can make inferences from it?</p>
<p>WLLN states that the mean of a sequence of i.i.d. random variables converges in probability to the expected value of the random variable as the length of that sequence tends to infinity. By ‘converging in probability’, we mean that the probability that the difference between the mean of the sample and the expected value of the random variable tends to zero.</p>
<p>In short, WLLN guarantees that with a large enough sample size the sample mean should approximately match the true population parameter. Clearly, this is powerful theorem for any statistical exercise: given we are (always) constrained by a finite sample, WLLN ensures that we can infer from the data something meaningful about the population. For example, from a large enough sample of voters we can estimate the average support for a candidate or party.</p>
<p>More formally, we can state WLLN as follows:</p>
<p><span class="math display">\[\begin{equation}
\bar{X}_n \xrightarrow{p} \mathbb{E}[X],
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\xrightarrow{p}\)</span> denotes `converging in probability’.</p>
</div>
<div id="proof_wlln" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Proof</h3>
<p>To prove WLLN, we use Chebyshev’s Inequality (CI). More specifically we first have to prove Chebyshev’s Inequality of the Sample Mean (CISM), and then use CISM to prove WLLN. The following steps are based on the proof provided in <span class="citation">Aronow and Miller (<a href="#ref-aronow2019foundations" role="doc-biblioref">2019</a>)</span>.</p>
<p><strong>Proof of Chebyshev’s Inequality of the Sample Mean.</strong> Chebyshev’s Inequality for the Sample Mean (CISM) states that:</p>
<p><span class="math display">\[\begin{equation}
 P(|\bar{X}_n-\mathbb{E}[X]| \geq k) \leq \frac{var(X)}{k^2 n},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\bar{X}_n\)</span> is the sample mean of a sequensce of <span class="math inline">\(n\)</span> independent draws from a random variable <span class="math inline">\(X\)</span>. Recall CI states that <span class="math inline">\(P(|(X-\mu)/\sigma| \geq k) \leq \frac{1}{k^2}\)</span>. To help prove CISM, we can rearrange the left hand side of the inequality by multiplying both sides of the inequality within the probability function by <span class="math inline">\(\sigma\)</span>, such that:</p>
<p><span class="math display">\[\begin{equation}
   P(|(X-\mu)| \geq k \sigma) \leq \frac{1}{k^2}.
\end{equation}\]</span></p>
<p>Then, finally, let us define <span class="math inline">\(k&#39; = \frac{k}{\sigma}\)</span>. Hence:</p>
<p><span class="math display">\[\begin{align}
  P(|(\bar{X}-\mathbb{E}[X])| \geq k) &amp;= P(|(\bar{X}-\mathbb{E}[X])| \geq k&#39;\sigma) \\
  &amp;\leq \frac{1}{{k&#39;}^2} \\
  &amp;\leq \frac{\sigma^2}{k^2} \\ 
  &amp;\leq \frac{var(\bar{X})}{k^2} \\
  &amp;\leq \frac{var(X)}{k^2 n} \; \; \; \square
\end{align}\]</span></p>
<p>This proof is reasonably straightfoward. Using our definition of <span class="math inline">\(k&#39;\)</span> allows us to us rearrange the probability within CISM to match the form of the Chebyshev Inequality stated above, which then allows us to infer the bounds of the probability. We then replace <span class="math inline">\(k&#39;\)</span> with <span class="math inline">\(\frac{k}{\sigma}\)</span>, expand and simplify. The move made between the penultimate and final line relies on the fact that variance of the sample mean is equal to the variance in the random variable divided by the sample size (n).</p>
<p><strong>Applying CISM to WLLN proof.</strong> Given that all probabilities are non-negative and CISM, we can now write:</p>
<p><span class="math display">\[\begin{equation}
  0 \leq P(|\bar{X}_n−\mathbb{E}[X]| \geq k) \leq \frac{var(X)}{k^2n}. \label{eq:multi_ineq}
\end{equation}\]</span></p>
<p>Note that for the first and third term of this multiple inequality, as <span class="math inline">\(n\)</span> approaches infinity both terms approach 0. In the case of the constant zero, this is trivial. In the final term, note that <span class="math inline">\(var(X)\)</span> denotes the inherent variance of the random variable, and therefore is constant as <span class="math inline">\(n\)</span> increases. Therefore, as the denominator increases, the term converges to zero.</p>
<p>Since the middle term is sandwiched in between these two limits, by definition we know that this term must also converge to zero. Therefore:</p>
<p><span class="math display">\[\begin{equation}
\text{lim}_{n \to \infty} P(|\bar{X}_{n}−\mathbb{E}[X]| \geq k) = 0 \; \; \; \square
\end{equation}\]</span></p>
<p>Hence, WLLN is proved: for any value of <span class="math inline">\(k\)</span>, the probability that the difference between the sample mean and the expected value is greater or equal to <span class="math inline">\(k\)</span> converges on zero. Since <span class="math inline">\(k\)</span>’s value is arbitrary, it can be set to something infinitesimally small, such that the sample mean and expected value converge in value.</p>
</div>
</div>
<div id="clt" class="section level2">
<h2><span class="header-section-number">4.2</span> Central Limit Theorem</h2>
<p>WLLN applies to the value of the statistic itself (the mean value). Given a <em>single</em>, n-length sequence drawn from a random variable, we know that the mean of this sequence will converge on the expected value of the random variable. But often, we want to think about what happens when we (hypothetically) calculate the mean across <em>multiple</em> sequences i.e. expectations under repeat sampling.</p>
<p>The Central Limit Theorem (CLT) is closely related to the WLLN. Like WLLN, it relies on asymptotic properties of random variables as the sample size increases. CLT, however, lets us make informative claims about the distribution of the sample mean around the true population parameter.</p>
<div id="theorem-in-plain-english-1" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Theorem in Plain English</h3>
<p>CLT states that as the sample size increases, the distribution of sample means converges to a normal distribution. That is, so long as the underlying distribution has a finite variance (bye bye Cauchy!), then irrespective of the underlying distribution of <span class="math inline">\(X\)</span> the distribution of sample means will be a normal distribution!</p>
<p>In fact, there are multiple types of CLT that apply in a variety of different contexts – cases including Bernoulli random variables (de Moivre - Laplace), where random variables are independent but do not need to be identically distributed (Lyapunov), and where random variables are vectors in <span class="math inline">\(\mathbb{R}^k\)</span> space (multivariate CLT).</p>
<p>In what follows, I will discuss a weaker, more basic case of CLT where we assume random variables are scalar, independent, and identically distributed (i.e. drawn from the same unknown distribution function). In particular, this section proves that the standardized difference between the sample mean and population mean for i.i.d. random variables converges in distribution to the standard normal distribution <span class="math inline">\(N(0,1)\)</span>. This variant of the CLT is called the Lindeberg-Levy CLT, and can be stated as:</p>
<p><span class="math display">\[\begin{equation}
  \frac{\bar{X}_n - \mu}{\sigma\sqrt{n}} \xrightarrow{d} N(0,1),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\xrightarrow{d}\)</span> denotes ‘converging in distribution’.</p>
<p>In general, the CLT is useful because proving that the sample mean is normally distributed allows us to quantify the uncertainty around our parameter estimate. Normal distributions have convenient properties that allow us to calculate the area under any portion of the curve, given just the same mean and standard deviation. We already know by WLLN that the sample mean will (with a sufficiently large sample) approximate the population mean, so we know that the distribution is also centred around the true population mean. By CLT, the dispersion around that point is therefore normal, and to quantify the probable bounds of the point estimate (under the assumption of repeat sampling) requires only an estimate of the variance.</p>
</div>
<div id="primer-characteristic-functions" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Primer: Characteristic Functions</h3>
<p>CLT is harder (and lengthier) to prove than other proofs we’ve encountered so far – it relies on showing that the sample mean converges <em>in distribution</em> to a known mathematical form that uniquely and fully describes the normal distribution. To do so, we use the idea of a <em>characteristic functions</em>, which simply denotes a function that completely defines a probability function.</p>
<p>For example, and we will use this later on, we know that the characteristic function of the normal distribution is <span class="math inline">\(e^{it\mu-\frac{\sigma^{2}t^{2}}{2}}\)</span>. A standard normal distriibution (where <span class="math inline">\(\mu = 0, \sigma^2 = 1\)</span>) therefore simplifies to <span class="math inline">\(e^{-\frac{t^{2}}{2}}\)</span>.</p>
<p>More generally, we know that for any scalar random variable <span class="math inline">\(X\)</span>, the characteristic function of <span class="math inline">\(X\)</span> is defined as:</p>
<p><span class="math display">\[\begin{equation}
\phi_X(t) = \mathbb{E}[e^{itX}],
\end{equation}\]</span></p>
<p>where <span class="math inline">\(t \in \mathbb{R}\)</span> and <span class="math inline">\(i\)</span> is the imaginary unit. Proving why this is the case is beyond the purview of this section, so unfortunately I will just take it at face value.</p>
<p>We can expand <span class="math inline">\(e^{itX}\)</span> as an infinite sum, using a Taylor Series, since <span class="math inline">\(e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + ...\)</span>. Hence:</p>
<p><span class="math display">\[\begin{equation}
\phi_X(t) = \mathbb{E}[1 + itX + \frac{(itX)^2}{2!} + \frac{(itX)^3}{3!} + ... ],
\end{equation}\]</span></p>
<p>Note that <span class="math inline">\(i^2 = -1\)</span>, and since the latter terms tend to zero faster than the second order term we can summarise them as <span class="math inline">\(o(t^2)\)</span> (they are no larger than of order <span class="math inline">\(t^2\)</span>). Therefore we can rewrite this expression as:</p>
<p><span class="math display">\[\begin{equation}
\phi_X(t) = \mathbb{E}[1 + itX - \frac{t^2}{2}X^2 + o(t^2)].
\end{equation}\]</span></p>
<p>In the case of continuous random variables, the expected value can be expressed as the integral across all space of the expression multiplied by the probability density, such that:</p>
<p><span class="math display">\[\begin{equation}
\phi_X(t) = \int^{\infty}_{-\infty}[1 + itX - \frac{t^2}{2}X^2 + o(t^2)]f_X dX,
\end{equation}\]</span></p>
<p>and this can be simplified to:</p>
<p><span class="math display">\[\begin{equation}
\phi_X(t) = 1 + it\mathbb{E}[X] - \frac{t^2}{2}\mathbb{E}[X^2] + o(t^2)],
\end{equation}\]</span></p>
<p>since <span class="math inline">\(1 \times f_X = f_X\)</span>, the total area under a probability density necessarily sums to 1; <span class="math inline">\(\int Xf_X dX\)</span> is the definition of the expected value of X, and so by similar logic <span class="math inline">\(\int X^2f_X dX = \mathbb{E}[X^2]\)</span>.</p>
<p>In <a href="https://www.youtube.com/watch?v=0oHjbr2_AhQ">Ben Lambert’s video</a> introducing the CLT proof, he notes that if we assume X has mean 0 and variance 1, the characteristic function of that distribution has some nice properties, namely that it simplifies to:</p>
<p><span class="math display">\[\begin{equation}
\phi_X(t) = 1 - \frac{t^2}{2} + o(t^2)],
\end{equation}\]</span></p>
<p>since <span class="math inline">\(\mathbb{E}[X] = 0\)</span> cancelling the second term, and <span class="math inline">\(\mathbb{E}[X^2] \equiv \mathbb{E}[(X-0)^2] = \mathbb{E}[(X-\mu)^2] = var(X) = 1\)</span>.</p>
<p>One final piece of characteristic function math that will help finalise the CLT proof is to note that if we define some random variable <span class="math inline">\(Q_n = \sum_{i=1}^{n}R_i\)</span>, where all <span class="math inline">\(R_i\)</span> are i.i.d., then the characteristic function of <span class="math inline">\(Q_n\)</span> can be expressed as <span class="math inline">\(\phi_{Q_n} (t) = [\phi_{R}(t)]^n\)</span>. Again, I will not prove this property here.</p>
</div>
<div id="proof-of-clt" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Proof of CLT</h3>
<p>This proof is based in part on Ben Lambert’s excellent <a href="https://www.youtube.com/channel/UC3tFZR3eL1bDY8CqZDOQh-w">YouTube series</a>, as well as <span class="citation">Lemons, Langevin, and Gythiel (<a href="#ref-Lemons_Langevin_Gythiel_2002" role="doc-biblioref">2002</a>)</span>.</p>
<p>Given the above discussion of a characteristic function, let us assume a sequence of independent and identically distributed (i.i.d.) random variables <span class="math inline">\({X_1, X_2, ..., X_n}\)</span>, each with mean <span class="math inline">\(\mu\)</span> and finite variance <span class="math inline">\(\sigma^2\)</span>. The sum of these random variables has mean <span class="math inline">\(n\mu\)</span> (since each random variable has the same mean) and the variance equivalent to <span class="math inline">\(n\sigma^2\)</span> (because the random variables are i.i.d. we know that <span class="math inline">\(var(A,B) = var(A)var(B)\)</span>).</p>
<p>Now let’s consider the standardized difference between the actual sum of the random variables and the mean. Standardization simply means dividing a parameter estimate by its standard deviation. In particular, we can consider the following standardized random variable:</p>
<p><span class="math display">\[\begin{equation}
Z_n = \frac{\sum_{i=1}^{n}(X_i - \mu)}{\sigma\sqrt{n}},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(Z_n\)</span>, in words, is the standardised difference between the sum of i.i.d. random variables and the expected value of the sequence. Note that we use the known variance in the denominator.</p>
<p>We can simplify this further:</p>
<p><span class="math display">\[\begin{equation}
Z_n = \sum_{i=1}^{n}\frac{1}{\sqrt{n}}Y_i,
\end{equation}\]</span></p>
<p>where we define a new random variable <span class="math inline">\(Y_i = \frac{X_i - \mu}{\sigma}.\)</span></p>
<p><span class="math inline">\(Y_i\)</span> has some convenient properties. First, since each random variable <span class="math inline">\(X_i\)</span> in our sample has mean <span class="math inline">\(\mu\)</span>, we know that <span class="math inline">\(\mathbb{E}[Y_i] = 0\)</span> since <span class="math inline">\(\mathbb{E}[X_i] = \mu\)</span> and therefore <span class="math inline">\(\mu - \mu = 0\)</span>. Note that this holds irrespective of the distribution and value of <span class="math inline">\(\mathbb{E}[X_i]\)</span>.</p>
<p>The variance of <span class="math inline">\(Y_i\)</span> is also recoverable. First note three basic features of variance: if <span class="math inline">\(a\)</span> is a constant, and <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables, <span class="math inline">\(var(a) = 0\)</span>; <span class="math inline">\(var(aX) = a^2var(X)\)</span>; and from the variance of a sum <span class="math inline">\(var(X - Y) = var(A)-var(B)\)</span>. Therefore:</p>
<p><span class="math display">\[\begin{align}
var(\frac{1}{\sigma}(X_i - \mu) &amp;= \frac{1}{\sigma^2}var(X_i - \mu) \\
var(X_i - \mu) &amp;= var(X_i) - var(\mu) \\
&amp;= var(X_i).
\end{align}\]</span></p>
<p>Hence:</p>
<p><span class="math display">\[\begin{equation}
var(Y_i) = \frac{var(X_i)}{\sigma^2} = 1,
\end{equation}\]</span></p>
<p>since <span class="math inline">\(var(X_i) = \sigma^2\)</span>.</p>
<p>At this stage, the proof is tantalisingly close. While we have not yet fully characterised the distribution of <span class="math inline">\(Z_n\)</span> or even <span class="math inline">\(Y_i\)</span>, the fact that <span class="math inline">\(Y_i\)</span> has unit variance and a mean of zero means suggests we are on the right track to proving that this does asymptotically tend in distribution to the standard normal. In fact, recall from the primer on characteristic functions, that Lambert notes for any random variable with unit variance and mean of 0, <span class="math inline">\(\phi_X(t) = 1 - \frac{t^2}{2} + o(t^2)\)</span>. Hence, we can now say that:</p>
<p><span class="math display">\[\begin{equation}
\phi_{Y_i}(t) = 1 - \frac{t^2}{2} + o(t^2). \label{eq:cf_yi_simple}
\end{equation}\]</span></p>
<p>Now let us return to <span class="math inline">\(Z_n = \sum_{i=1}^{n}\frac{1}{\sqrt{n}}Y_i\)</span> and using the final bit of characteristic function math in the primer, we can express the characteristic function of <span class="math inline">\(Z_n\)</span> as:</p>
<p><span class="math display">\[\begin{equation}
\phi_{Z_n} (t) = [\phi_{Y} (\frac{t}{\sqrt{n}})]^n,
\end{equation}\]</span></p>
<p>since <span class="math inline">\(Y_i\)</span> is divided by the square root of the sample size. Given our previously stated expression of the characteristic function of <span class="math inline">\(Y_i\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\phi_{Z_n} (t) = [1 - \frac{t^2}{2n} + o(t^2)]]^n.
\end{equation}\]</span></p>
<p>We can now consider what happens as <span class="math inline">\(n \to \infty\)</span>. By definition, we know that <span class="math inline">\(o(t^2)\)</span> converges to zero faster than the other terms, so we can safely ignore it. As a result, and noting that <span class="math inline">\(e^x = \lim(1+\frac{x}{n})^n\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\lim_{n \to \infty}\phi_{Z_n} (t) = e^{-\frac{t^2}{2}}.
\end{equation}\]</span></p>
<p>This expression shows that as <span class="math inline">\(n\)</span> tends to infinity, the characteristic function of <span class="math inline">\(Z_n\)</span> is the standard normal distribution (as noted in the characteristic function primer). Therefore:</p>
<p><span class="math display">\[\begin{align}
\lim_{n \to \infty}Z_n &amp;= N(0,1) \\
\lim_{n \to \infty}\frac{\bar{X}_n - \mu}{\sigma\sqrt{n}} &amp;= N(0,1). \; \; \; \square
\end{align}\]</span></p>
<p>The last line here simply follows from the definition of <span class="math inline">\(Z_n\)</span>.</p>
</div>
<div id="generalising-clt" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Generalising CLT</h3>
<p>From here, it is possible to intuit the more general CLT that the distribution of sampling means is normally distributed around the true mean <span class="math inline">\(\mu\)</span> with variance <span class="math inline">\(\frac{\sigma^2}{n}\)</span>. Note this is only a pseudo-proof, because as Lambert notes, multiplying through by <span class="math inline">\(n\)</span> is complicated by the limit operator with respect to <span class="math inline">\(n\)</span>. However, it is useful to see how these two CLT are closely related.</p>
<p>First, we can rearrange the limit expression using known features of the normal distribution:</p>
<p><span class="math display">\[\begin{align}
\lim_{n \to \infty}Z_n &amp; \xrightarrow{d} N(0,1) \\
\lim_{n \to \infty} \frac{\sum_{i=1}^{n}(X_i) - n\mu}{\sqrt{n\sigma^2}} &amp; \xrightarrow{d} N(0,1) \\
\lim_{n \to \infty} \sum_{i=1}^{n}(X_i) - n\mu &amp; \xrightarrow{d} N(0,n\sigma^2) \\
\lim_{n \to \infty} \sum_{i=1}^{n}(X_i) &amp; \xrightarrow{d} N(n\mu,n\sigma^2),
\end{align}\]</span></p>
<p>since <span class="math inline">\(aN(b,c) = N(ab, a^2c),\)</span> and <span class="math inline">\(N(d,e) + f = N(d+f, e).\)</span></p>
<p>At this penultimate step, we know that the sum of i.i.d. random variables is a normal distribution. To see that the sample mean is also normally distributed, we simply divide through by <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\lim_{n \to \infty} \bar{X} = \frac{1}{n}\sum_{i=1}^{n}(X_i) \xrightarrow{d} N(\mu,\frac{\sigma^2}{n}).
\end{equation}\]</span></p>
</div>
<div id="limitation-of-clt-and-the-importance-of-wlln" class="section level3">
<h3><span class="header-section-number">4.2.5</span> Limitation of CLT (and the importance of WLLN)</h3>
<p>Before ending, it is worth noting that CLT is a claim with respect to repeat sampling from a population (holding <span class="math inline">\(n\)</span> constant each time). It is not, therefore, a claim that holds with respect to any particular sample draw. We may actually estimate a mean value that, while probable, lies away from the true population parameter (by definition, since the sample means are normally distributed, there is some dispersion). Constructing uncertainty estimates using CLT on this estimate alone does not guarantee that we are in fact capturing <em>either</em> the true variance or the true parameter.</p>
<p>That being said, with sufficiently high-N, we know that WLLN guarantees (assuming i.i.d. observations) that our estimate converges on the population mean. WLLN’s asymptotics rely only on sufficiently large sample sizes for a <em>single</em> sample. Hence, both WLLN and CLT are crucial for valid inference from sampled data. WLLN leads us to expect that our parameter estimate will in fact be centred approximately near the true parameter. Here, CLT can only say that across multiple samples from the population the distribution of sample means is centred on the true parameter. With WLLN in action, however, CLT allows us to make inferential claims about the uncertainty of this converged parameter.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-aronow2019foundations">
<p>Aronow, P. M., and B. T. Miller. 2019. <em>Foundations of Agnostic Statistics</em>. Cambridge University Press. <a href="https://books.google.co.uk/books?id=u1N-DwAAQBAJ">https://books.google.co.uk/books?id=u1N-DwAAQBAJ</a>.</p>
</div>
<div id="ref-Lemons_Langevin_Gythiel_2002">
<p>Lemons, D. S., P. Langevin, and A. Gythiel. 2002. <em>An Introduction to Stochastic Processes in Physics</em>. Johns Hopkins Paperback. Johns Hopkins University Press. <a href="https://books.google.co.uk/books?id=Uw6YDkd_CXcC">https://books.google.co.uk/books?id=Uw6YDkd_CXcC</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-projection.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="slutsky.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05-wlln.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["10EconometricTheorems.pdf", "10EconometricTheorems.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
