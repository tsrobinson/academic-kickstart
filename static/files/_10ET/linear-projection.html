<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Linear Projection | 10 Fundamental Theorems for Econometrics</title>
  <meta name="description" content="This book walks through the ten most important statistical theorems as highlighted by Jeffrey Wooldridge, presenting intuiitions, proofs, and applications." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Linear Projection | 10 Fundamental Theorems for Econometrics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book walks through the ten most important statistical theorems as highlighted by Jeffrey Wooldridge, presenting intuiitions, proofs, and applications." />
  <meta name="github-repo" content="tsrobinson/10EconTheorems" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Linear Projection | 10 Fundamental Theorems for Econometrics" />
  
  <meta name="twitter:description" content="This book walks through the ten most important statistical theorems as highlighted by Jeffrey Wooldridge, presenting intuiitions, proofs, and applications." />
  

<meta name="author" content="Thomas S. Robinson  (https://ts-robinson.com)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="exp-ineq.html"/>
<link rel="next" href="wlln.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">10 Fundamental Theorems for Econometrics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version-notes"><i class="fa fa-check"></i>Version notes</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="exp-theorems.html"><a href="exp-theorems.html"><i class="fa fa-check"></i><b>1</b> Expectation Theorems</a><ul>
<li class="chapter" data-level="1.1" data-path="exp-theorems.html"><a href="exp-theorems.html#law-of-iterated-expectations"><i class="fa fa-check"></i><b>1.1</b> Law of Iterated Expectations</a><ul>
<li class="chapter" data-level="1.1.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-lie"><i class="fa fa-check"></i><b>1.1.1</b> Proof of LIE</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="exp-theorems.html"><a href="exp-theorems.html#law-of-total-variance"><i class="fa fa-check"></i><b>1.2</b> Law of Total Variance</a><ul>
<li class="chapter" data-level="1.2.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-ltv"><i class="fa fa-check"></i><b>1.2.1</b> Proof of LTV</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="exp-theorems.html"><a href="exp-theorems.html#linearity-of-expectations"><i class="fa fa-check"></i><b>1.3</b> Linearity of Expectations</a><ul>
<li class="chapter" data-level="1.3.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-loe"><i class="fa fa-check"></i><b>1.3.1</b> Proof of LOE</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exp-theorems.html"><a href="exp-theorems.html#variance-of-a-sum"><i class="fa fa-check"></i><b>1.4</b> Variance of a Sum</a><ul>
<li class="chapter" data-level="1.4.1" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-vos-x-y-are-independent"><i class="fa fa-check"></i><b>1.4.1</b> Proof of VoS: <span class="math inline">\(X, Y\)</span> are independent</a></li>
<li class="chapter" data-level="1.4.2" data-path="exp-theorems.html"><a href="exp-theorems.html#proof-of-vos-x-y-are-dependent"><i class="fa fa-check"></i><b>1.4.2</b> Proof of VoS: <span class="math inline">\(X, Y\)</span> are dependent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exp-ineq.html"><a href="exp-ineq.html"><i class="fa fa-check"></i><b>2</b> Inequalities involving expectations</a><ul>
<li class="chapter" data-level="2.1" data-path="exp-ineq.html"><a href="exp-ineq.html#jensens-inequality"><i class="fa fa-check"></i><b>2.1</b> Jensen’s Inequality</a><ul>
<li class="chapter" data-level="2.1.1" data-path="exp-ineq.html"><a href="exp-ineq.html#convex-functions"><i class="fa fa-check"></i><b>2.1.1</b> Convex functions</a></li>
<li class="chapter" data-level="2.1.2" data-path="exp-ineq.html"><a href="exp-ineq.html#the-inequality"><i class="fa fa-check"></i><b>2.1.2</b> The Inequality</a></li>
<li class="chapter" data-level="2.1.3" data-path="exp-ineq.html"><a href="exp-ineq.html#proof_ji"><i class="fa fa-check"></i><b>2.1.3</b> Proof</a></li>
<li class="chapter" data-level="2.1.4" data-path="exp-ineq.html"><a href="exp-ineq.html#application"><i class="fa fa-check"></i><b>2.1.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="exp-ineq.html"><a href="exp-ineq.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>2.2</b> Chebyshev’s Inequality</a><ul>
<li class="chapter" data-level="2.2.1" data-path="exp-ineq.html"><a href="exp-ineq.html#proof"><i class="fa fa-check"></i><b>2.2.1</b> Proof</a></li>
<li class="chapter" data-level="2.2.2" data-path="exp-ineq.html"><a href="exp-ineq.html#applications"><i class="fa fa-check"></i><b>2.2.2</b> Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-projection.html"><a href="linear-projection.html"><i class="fa fa-check"></i><b>3</b> Linear Projection</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-projection.html"><a href="linear-projection.html#proj_def"><i class="fa fa-check"></i><b>3.1</b> Projection</a></li>
<li class="chapter" data-level="3.2" data-path="linear-projection.html"><a href="linear-projection.html#proj_prop"><i class="fa fa-check"></i><b>3.2</b> Properties of the projection matrix</a></li>
<li class="chapter" data-level="3.3" data-path="linear-projection.html"><a href="linear-projection.html#lp_lr"><i class="fa fa-check"></i><b>3.3</b> Linear regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="linear-projection.html"><a href="linear-projection.html#geometric-interpretation"><i class="fa fa-check"></i><b>3.3.1</b> Geometric interpretation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="wlln.html"><a href="wlln.html"><i class="fa fa-check"></i><b>4</b> Weak Law of Large Numbers and Central Limit Theorem</a><ul>
<li class="chapter" data-level="4.1" data-path="wlln.html"><a href="wlln.html#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>4.1</b> Weak Law of Large Numbers</a><ul>
<li class="chapter" data-level="4.1.1" data-path="wlln.html"><a href="wlln.html#theorem-in-plain-english"><i class="fa fa-check"></i><b>4.1.1</b> Theorem in Plain English</a></li>
<li class="chapter" data-level="4.1.2" data-path="wlln.html"><a href="wlln.html#proof_wlln"><i class="fa fa-check"></i><b>4.1.2</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="wlln.html"><a href="wlln.html#clt"><i class="fa fa-check"></i><b>4.2</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="4.2.1" data-path="wlln.html"><a href="wlln.html#theorem-in-plain-english-1"><i class="fa fa-check"></i><b>4.2.1</b> Theorem in Plain English</a></li>
<li class="chapter" data-level="4.2.2" data-path="wlln.html"><a href="wlln.html#primer-characteristic-functions"><i class="fa fa-check"></i><b>4.2.2</b> Primer: Characteristic Functions</a></li>
<li class="chapter" data-level="4.2.3" data-path="wlln.html"><a href="wlln.html#proof-of-clt"><i class="fa fa-check"></i><b>4.2.3</b> Proof of CLT</a></li>
<li class="chapter" data-level="4.2.4" data-path="wlln.html"><a href="wlln.html#generalising-clt"><i class="fa fa-check"></i><b>4.2.4</b> Generalising CLT</a></li>
<li class="chapter" data-level="4.2.5" data-path="wlln.html"><a href="wlln.html#limitation-of-clt-and-the-importance-of-wlln"><i class="fa fa-check"></i><b>4.2.5</b> Limitation of CLT (and the importance of WLLN)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="slutsky.html"><a href="slutsky.html"><i class="fa fa-check"></i><b>5</b> Slutsky’s Theorem</a><ul>
<li class="chapter" data-level="5.1" data-path="slutsky.html"><a href="slutsky.html#theorem_slutsky"><i class="fa fa-check"></i><b>5.1</b> Theorem in plain English</a></li>
<li class="chapter" data-level="5.2" data-path="slutsky.html"><a href="slutsky.html#coded-demonstration"><i class="fa fa-check"></i><b>5.2</b> Coded demonstration</a></li>
<li class="chapter" data-level="5.3" data-path="slutsky.html"><a href="slutsky.html#proof-of-slutskys-theorem"><i class="fa fa-check"></i><b>5.3</b> Proof of Slutsky’s Theorem</a><ul>
<li class="chapter" data-level="5.3.1" data-path="slutsky.html"><a href="slutsky.html#cmt"><i class="fa fa-check"></i><b>5.3.1</b> CMT</a></li>
<li class="chapter" data-level="5.3.2" data-path="slutsky.html"><a href="slutsky.html#proof-using-cmt"><i class="fa fa-check"></i><b>5.3.2</b> Proof using CMT</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="slutsky.html"><a href="slutsky.html#applications_slutsky"><i class="fa fa-check"></i><b>5.4</b> Applications</a><ul>
<li class="chapter" data-level="5.4.1" data-path="slutsky.html"><a href="slutsky.html#proving-the-consistency-of-sample-variance-and-the-normality-of-the-t-statistic"><i class="fa fa-check"></i><b>5.4.1</b> Proving the consistency of sample variance, and the normality of the t-statistic</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html"><i class="fa fa-check"></i><b>6</b> Big Op and little op</a><ul>
<li class="chapter" data-level="6.1" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#stochastic-order-notation"><i class="fa fa-check"></i><b>6.1</b> Stochastic order notation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#relationship-of-big-o-and-little-o"><i class="fa fa-check"></i><b>6.1.1</b> Relationship of big-O and little-o</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#notational-shorthand-and-arithmetic-properties"><i class="fa fa-check"></i><b>6.2</b> Notational shorthand and ``arithmetic" properties</a></li>
<li class="chapter" data-level="6.3" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#why-is-this-usefulfn_cite1"><i class="fa fa-check"></i><b>6.3</b> Why is this useful?</a></li>
<li class="chapter" data-level="6.4" data-path="big-op-and-little-op.html"><a href="big-op-and-little-op.html#estimator_consistency"><i class="fa fa-check"></i><b>6.4</b> Worked Example: Consistency of mean estimators</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="dm.html"><a href="dm.html"><i class="fa fa-check"></i><b>7</b> Delta Method</a><ul>
<li class="chapter" data-level="7.1" data-path="dm.html"><a href="dm.html#delta-method-in-plain-english"><i class="fa fa-check"></i><b>7.1</b> Delta Method in Plain English</a></li>
<li class="chapter" data-level="7.2" data-path="dm.html"><a href="dm.html#proof_dm"><i class="fa fa-check"></i><b>7.2</b> Proof</a><ul>
<li class="chapter" data-level="7.2.1" data-path="dm.html"><a href="dm.html#taylors-series-and-theorem"><i class="fa fa-check"></i><b>7.2.1</b> Taylor’s Series and Theorem</a></li>
<li class="chapter" data-level="7.2.2" data-path="dm.html"><a href="dm.html#proof-of-delta-method"><i class="fa fa-check"></i><b>7.2.2</b> Proof of Delta Method</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="dm.html"><a href="dm.html#applied-example"><i class="fa fa-check"></i><b>7.3</b> Applied example</a></li>
<li class="chapter" data-level="7.4" data-path="dm.html"><a href="dm.html#alternative-strategies"><i class="fa fa-check"></i><b>7.4</b> Alternative strategies</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="frisch.html"><a href="frisch.html"><i class="fa fa-check"></i><b>8</b> Frisch-Waugh-Lovell Theorem</a><ul>
<li class="chapter" data-level="8.1" data-path="wlln.html"><a href="wlln.html#theorem-in-plain-english"><i class="fa fa-check"></i><b>8.1</b> Theorem in plain English</a></li>
<li class="chapter" data-level="8.2" data-path="frisch.html"><a href="frisch.html#proof_fw"><i class="fa fa-check"></i><b>8.2</b> Proof</a><ul>
<li class="chapter" data-level="8.2.1" data-path="frisch.html"><a href="frisch.html#primer-projection-matricessecnote"><i class="fa fa-check"></i><b>8.2.1</b> Primer: Projection matrices</a></li>
<li class="chapter" data-level="8.2.2" data-path="frisch.html"><a href="frisch.html#fwl-proof-secnote2"><i class="fa fa-check"></i><b>8.2.2</b> FWL Proof</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="frisch.html"><a href="frisch.html#coded-example"><i class="fa fa-check"></i><b>8.3</b> Coded example</a></li>
<li class="chapter" data-level="8.4" data-path="frisch.html"><a href="frisch.html#application-sensitivity-analysis"><i class="fa fa-check"></i><b>8.4</b> Application: Sensitivity analysis</a><ul>
<li class="chapter" data-level="8.4.1" data-path="frisch.html"><a href="frisch.html#regressing-the-partialled-out-x-on-the-full-y"><i class="fa fa-check"></i><b>8.4.1</b> Regressing the partialled-out X on the full Y</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="pd.html"><a href="pd.html"><i class="fa fa-check"></i><b>9</b> Positive Definite Matrices</a><ul>
<li class="chapter" data-level="9.1" data-path="pd.html"><a href="pd.html#pd_terms"><i class="fa fa-check"></i><b>9.1</b> Terminology</a><ul>
<li class="chapter" data-level="9.1.1" data-path="pd.html"><a href="pd.html#positivity"><i class="fa fa-check"></i><b>9.1.1</b> Positivity</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="pd.html"><a href="pd.html#a-b-is-psd-iff-b-1---a-1-is-psd"><i class="fa fa-check"></i><b>9.2</b> <span class="math inline">\(A-B\)</span> is PSD iff <span class="math inline">\(B^{-1} - A^{-1}\)</span> is PSD</a><ul>
<li class="chapter" data-level="9.2.1" data-path="pd.html"><a href="pd.html#pd_proof"><i class="fa fa-check"></i><b>9.2.1</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="pd.html"><a href="pd.html#pd_app"><i class="fa fa-check"></i><b>9.3</b> Applications</a><ul>
<li class="chapter" data-level="9.3.1" data-path="pd.html"><a href="pd.html#ols-as-the-best-linear-unbiased-estimator-blue"><i class="fa fa-check"></i><b>9.3.1</b> OLS as the best linear unbiased estimator (BLUE)</a></li>
<li class="chapter" data-level="9.3.2" data-path="pd.html"><a href="pd.html#optimisation-problems"><i class="fa fa-check"></i><b>9.3.2</b> Optimisation problems</a></li>
<li class="chapter" data-level="9.3.3" data-path="pd.html"><a href="pd.html#recap"><i class="fa fa-check"></i><b>9.3.3</b> Recap</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">10 Fundamental Theorems for Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear_projection" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Linear Projection</h1>
<p>This chapter provides a basic introduction to projection using both linear algebra and geometric demonstrations. I discuss the derivation of the orthogonal projection, its general properties as an “operator”, and explore its relationship with ordinary least squares (OLS) regression. I defer a discussion of linear projections’ applications until the <a href="frisch.html#frisch">penultimate chapter</a> on the Frisch-Waugh Theorem, where projection matrices feature heavily in the proof.</p>
<div id="proj_def" class="section level2">
<h2><span class="header-section-number">3.1</span> Projection</h2>
<p>Formally, a projection <span class="math inline">\(P\)</span> is a linear function on a vector space, such that when it is applied to itself you get the same result i.e. <span class="math inline">\(P^2 = P\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>This definition is slightly intractable, but the intuition is reasonably simple. Consider a vector <span class="math inline">\(v\)</span> in two-dimensions. <span class="math inline">\(v\)</span> is a finite straight line pointing in a given direction. Suppose there is some point <span class="math inline">\(x\)</span> not on this straight line but in the same two-dimensional space. The projection of <span class="math inline">\(x\)</span>, i.e. <span class="math inline">\(Px\)</span>, is a function that returns the point “closest” to <span class="math inline">\(x\)</span> along the vector line <span class="math inline">\(v\)</span>. Call this point <span class="math inline">\(\bar{x}\)</span>. In most contexts, closest refers to Euclidean distance, i.e. <span class="math inline">\(\sqrt{\sum_{i} (x_i - \bar{x}_i)^2},\)</span> where <span class="math inline">\(i\)</span> ranges over the dimensions of the vector space (in this case two dimensions).[fn^euclid] Figure @ref(fig:lp_basic) depicts this logic visually. The green dashed line shows the orthogonal projection, and red dashed lines indicate other potential (non-orthgonal) projections that are further away in Euclidean space from <span class="math inline">\(x\)</span> than <span class="math inline">\(\bar{x}\)</span>.</p>
<div class="figure"><span id="fig:lp-basic"></span>
<img src="images/lp_basic.png" alt="Orthogonal projection of a point onto a vector line." width="100%" />
<p class="caption">
Figure 3.1: Orthogonal projection of a point onto a vector line.
</p>
</div>
<p>In short, projection is a way of simplifying some n-dimensional space – compressing information onto a (hyper-) plane. This is useful especially in social science settings where the complexity of the phenomena we study mean exact prediction is impossible. Instead, we often want to construct models that compress busy and variable data into simpler, parsimonious explanations. Projection is the statistical method of achieving this – it takes the full space and simplifies it with respect to a certain number of dimensions.</p>
<p>While the above is (reasonably) intuitive it is worth spelling out the maths behind projection, not least because it helps demonstrate the connection between linear projection and linear regression.</p>
<p>To begin, we can take some point in n-dimensional space, <span class="math inline">\(x\)</span>, and the vector line <span class="math inline">\(v\)</span> along which we want to project <span class="math inline">\(x\)</span>. The goal is the following:</p>
<p><span class="math display">\[
\begin{aligned}
{arg\,min}_c \sqrt{\sum_{i} (\bar{x}_i - x)^2} &amp; = {arg\,min}_c \sum_{i} (\bar{x}_i - x)^2 \\
&amp; = {arg\,min}_c \sum_{i} (cv_i - x)^2
\end{aligned}
\]</span></p>
<p>This rearrangement follows since the square root is a monotonic transformation, such that the optimal choice of <span class="math inline">\(c\)</span> is the same across both <span class="math inline">\(arg\;min\)</span>’s. Since any potential <span class="math inline">\(\bar{x}\)</span> along the line drawn by <span class="math inline">\(v\)</span> is some scalar multiplication of that line (<span class="math inline">\(cv\)</span>), we can express the function to be minimised with respect to <span class="math inline">\(c\)</span>, and then differentiate:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{d}{dc} \sum_{i} (cv_i - x)^2 &amp; = \sum_{i}2v_i(cv_i - x) \\
&amp; = 2(\sum_{i}cv_i^2  - \sum_{i}v_ix) \\
&amp; = 2(cv&#39;v - v&#39;x) \Rightarrow 0
\end{aligned}
\]</span></p>
<p>Here we differentiate the equation and rearrange terms. The final step simply converts the summation notation into matrix multiplication. Solving:</p>
<p><span class="math display">\[
\begin{aligned}
2(cv&#39;v - v&#39;x) &amp;= 0 \label{eq:dc_equal_zero} \\ 
cv&#39;v - v&#39;x &amp;= 0 \\
cv&#39;v &amp;= v&#39;x \\
c &amp;= (v&#39;v)^{-1}v&#39;x.
\end{aligned}
\]</span></p>
<p>From here, note that <span class="math inline">\(\bar{x}\)</span>, the projection of <span class="math inline">\(x\)</span> onto the vector line, is <span class="math inline">\(vc = v(v&#39;v)^{-1}v&#39;x.\)</span> Hence, we can define the projection matrix of <span class="math inline">\(x\)</span> onto <span class="math inline">\(v\)</span> as:</p>
<p><span class="math display">\[ P_v = v(v&#39;v)^{-1}v&#39;.\]</span></p>
<p>In plain English, for any point in some space, the orthogonal projection of that point onto some subspace, is the point on a vector line that minimises the Euclidian distance between itself and the original point. A visual demonstration of this point is shown and discussed in Figure <a href="#fig:lp-demo"><strong>??</strong></a> below.</p>
<p>Note also that this projection matrix has a clear analogue to the linear algebraic expression of linear regression. The vector of coefficients in a linear regression <span class="math inline">\(\hat{\beta}\)</span> can be expressed as <span class="math inline">\((X′X)^{-1}X′y\)</span>. And we know that multiplying this vector by the matrix of predictors <span class="math inline">\(X\)</span> results in the vector of predicted values <span class="math inline">\(\hat{y}\)</span>. Now we have <span class="math inline">\(\hat{y} = X(X&#39;X)^{-1}X&#39;Y \equiv P_Xy\)</span>. Clearly, therefore, linear projection and linear regression are closely related – and I return to this point <a href="linear-projection.html#lp_lr">below</a>.</p>
</div>
<div id="proj_prop" class="section level2">
<h2><span class="header-section-number">3.2</span> Properties of the projection matrix</h2>
<p>The projection matrix <span class="math inline">\(P\)</span> has several interesting properties. First, and most simply, the projection matrix is square. Since <span class="math inline">\(v\)</span> is of some arbitrary dimensions <span class="math inline">\(n \times k\)</span>, its transpose is of dimensions <span class="math inline">\(k \times n\)</span>. By linear algebra, the shape of the full matrix is therefore <span class="math inline">\(n \times n\)</span>, i.e. square.</p>
<p>Projection matrices are also symmetric, i.e. <span class="math inline">\(P = P&#39;\)</span>. To prove symmetry, note that transposing both sides of the projection matrix definition:</p>
<p><span class="math display">\[\begin{align}
P&#39; &amp;= (v(v&#39;v)^{-1}v&#39;)&#39; \\
&amp;= v(v&#39;v)^{-1}v&#39;\\
&amp;= P,
\end{align}\]</span></p>
<p>since <span class="math inline">\((AB)&#39;=B&#39;A&#39;\)</span> and <span class="math inline">\((A^{-1})&#39; = (A&#39;)^{-1}.\)</span></p>
<p>Projection matrices are idempotent:</p>
<p><span class="math display">\[\begin{align}
PP &amp;= v(v&#39;v)^{-1}v&#39;v(v&#39;v)^{-1}v&#39; \\
&amp;= v(v&#39;v)^{-1}v&#39;\\
&amp;= P,
\end{align}\]</span></p>
<p>since <span class="math inline">\((A)^{-1}A = I\)</span> and <span class="math inline">\(BI = B\)</span>.</p>
<p>Since, projection matrices are idempotent, this entails that projecting a point already on the vector line will just return that same point. This is fairly intuitive: the closest point on the vector line to a point already on the vector line is just that same point.</p>
<p>Finally, we can see that the projection of any point is orthogonal to the respected projected point on vector line. Two vectors are orthogonal if <span class="math inline">\(ab = 0\)</span>. Starting with the expression in Equation  (i.e. minimising the Euclidean distance with respect to <span class="math inline">\(c\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
2(cv&#39;v - v&#39;x) &amp;= 0 \\ 
v&#39;cv - v&#39;x &amp;= 0 \\
v&#39;(cv-x) &amp;= 0 \\
v&#39;(\bar{x} - x) &amp;= 0,
\end{aligned}
\]</span></p>
<p>hence the line connecting the original point <span class="math inline">\(x\)</span> is orthogonal to the vector line.</p>
<p>The projection matrix is very useful in other fundamental theorems in econometrics, like Frisch Waugh Lovell Theorem discussed in <a href="frisch.html#frisch">Chapter 8</a>.</p>
</div>
<div id="lp_lr" class="section level2">
<h2><span class="header-section-number">3.3</span> Linear regression</h2>
<p>Given a vector of interest, how do we capture as much information from it as possible using set of predictors? Projection matrices essentially simplify the dimensionality of some space, by casting points onto a lower-dimensional plane. Think of it like capturing the shadow of an object on the ground. There is far more detail in the actual object itself but we roughly know its position, shape, and scale from the shadow that’s cast on the 2d plane of the ground.</p>
<p>Note also this is actually quite similar to how we think about regression. Loosely, when we regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>, we are trying to characterise how the components (or predictors) within <span class="math inline">\(X\)</span> characterise or relate to <span class="math inline">\(Y\)</span>. Of course, regression is also imperfect (after all, the optimisation goal is to minimise the errors of our predictions). So, regression also seems to capture some lower dimensional approximation of an outcome.</p>
<p>In fact, linear projection and linear regression are very closely related. In this final section, I outline how these two statistical concepts relate to each other, both algebraically and geometrically,</p>
<p>Suppose we have a vector of outcomes <span class="math inline">\(y\)</span>, and some n-dimensional matrix <span class="math inline">\(X\)</span> of predictors. We write the linear regression model as:</p>
<p><span class="math display">\[\begin{equation}
y = X\beta + \epsilon,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is a vector of coefficients, and <span class="math inline">\(\epsilon\)</span> is the difference between the prediction and the observed value in <span class="math inline">\(y\)</span>. The goal of linear regression is to minimise the sum of the squared residuals:</p>
<p><span class="math display">\[
arg\,min \,\,  \epsilon^2 = arg\,min (y - X\beta)&#39;(y-X\beta)
\]</span></p>
<p>Differentiating with respect to and solving:
<span class="math display">\[
\begin{aligned}
\frac{d}{d\beta} (y - X\beta)&#39;(y-X\beta) &amp;= -2X(y - X\beta) \\
&amp;= 2X&#39;X\beta -2X&#39;y \Rightarrow 0 \\
X&#39;X\hat{\beta} &amp;= X&#39;y \\
(X&#39;X)^{-1}X&#39;X\hat{\beta} &amp;= (X&#39;X)^{-1}X&#39;y \\
\hat{\beta} &amp;= (X&#39;X)^{-1}X&#39;y.
\end{aligned}
\]</span></p>
<p>To get our prediction of <span class="math inline">\(y\)</span>, i.e. <span class="math inline">\(\hat{y}\)</span>, we simply multiply our beta coefficient by the matrix X:</p>
<p><span class="math display">\[
  \hat{y} = X(X&#39;X)^{-1}X&#39;y.
\]</span></p>
<p>Note how the OLS derivation of <span class="math inline">\(\hat{y}\)</span> is very similar to <span class="math inline">\(P = X(X&#39;X)^{-1}X\)</span>, the orthogonal prediction matrix. The two differ only in that that <span class="math inline">\(\hat{y}\)</span> includes the original outcome vector <span class="math inline">\(y\)</span> in its expression. But, note that <span class="math inline">\(Py = X(X&#39;X)^{-1}X&#39;y = \hat{y}\)</span>! Hence the predicted values from a linear regression simply are an orthogonal projection of <span class="math inline">\(y\)</span> onto the space defined by <span class="math inline">\(X\)</span>.</p>
<div id="geometric-interpretation" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Geometric interpretation</h3>
<p>It should be clear now that linear projection and linear regression are connected – but it is probably less clear <em>why</em> this holds. To understand what’s going on, let’s depict the problem geometrically.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>To appreciate what’s going on, we first need to invert how we typically think about observations, variables and datapoints. Consider a bivariate regression problem with three observations. Our data will include three variables: a constant (c, a vector of 1’s), a predictor (X), and an outcome variable (Y). As a matrix, this might look something like the following:</p>
<table>
<thead>
<tr class="header">
<th align="center">Y</th>
<th align="center">X</th>
<th align="center">c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">2</td>
<td align="center">3</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<p>Typically we would represent the relationship geometrically by treating the variables as dimensions, such that every datapoint is an observation (and we would typically ignore the constant column since all its values are the same).</p>
<p>An alternative way to represent this data is to treat each observation (i.e. row) as a dimension and then represent each variable as a vector. What does that actually mean? Well consider the column <span class="math inline">\(Y = (2,3,2)\)</span>. This vector essentially gives us the coordinates for a point in three-dimensional space: <span class="math inline">\(d_1 = 2, d_2 = 3, d_3 = 2\)</span>. Drawing a straight line from the origin (0,0,0) to this point gives us a vector line for the outcome. While visually this might seem strange, from the perspective of our data it’s not unusual to refer to each variable as a column vector, and that’s precisely because it is a quantity with a magnitude and direction (as determined by its position in <span class="math inline">\(n\)</span> dimensions).</p>
<p>Our predictors are the vectors <span class="math inline">\(X\)</span> and <span class="math inline">\(c\)</span> (note the vector <span class="math inline">\(c\)</span> is now slightly more interesting because it is a diagonal line through the three-dimensional space). We can extend either vector line by multiplying it by a constant e.g. <span class="math inline">\(2X = (6,2,2)\)</span>. With a single vector, we can only move forwards or backwards along a line. But if we combine two vectors together, we can actually reach lots of points in space. Imagine placing the vector <span class="math inline">\(X\)</span> at the end of the <span class="math inline">\(c\)</span>. The total path now reaches a new point that is not intersected by either <span class="math inline">\(X\)</span> or <span class="math inline">\(c\)</span>. In fact, if we multiply <span class="math inline">\(X\)</span> and <span class="math inline">\(c\)</span> by some scalars (numbers), we can snake our way across a whole array of different points in three-dimensional space. Figure <a href="linear-projection.html#fig:lp-span">3.2</a> demonstrates some of these combinations in the two dimensional space created by <span class="math inline">\(X\)</span> and <span class="math inline">\(c\)</span>.</p>
<div class="figure"><span id="fig:lp-span"></span>
<img src="images/span.png" alt="Potential combinations of two vectors." width="100%" />
<p class="caption">
Figure 3.2: Potential combinations of two vectors.
</p>
</div>
<p>The comprehensive set of all possible points covered by linear combinations of <span class="math inline">\(X\)</span> and <span class="math inline">\(c\)</span> is called the <em>span</em> or <em>column space</em>. In fact, with the specific set up of this example (3 observations, two predictors), the span of our predictors is a flat plane. Imagine taking a flat bit of paper and aligning one corner with the origin, and then angling surface so that the end points of the vectors <span class="math inline">\(X\)</span> and <span class="math inline">\(c\)</span> are both resting on the card’s surface. Keeping that alignment, any point on the surface of the card is reachable by some combination of <span class="math inline">\(X\)</span> and <span class="math inline">\(c\)</span>. Algebraically we can refer to this surface as <span class="math inline">\(col(X,c)\)</span>, and it generalises beyond two predictors (although this is much harder to visualise).</p>
<p>Crucially, in our reduced example of three-dimensional space, there are points in space not reachable by combining these two vectors (any point above or below the piece of card). We know, for instance that the vector line <span class="math inline">\(y\)</span> lies off this plane. The goal therefore is to find a vector that is on the column space of <span class="math inline">\((X,c)\)</span> that gets closest to our off-plane vector <span class="math inline">\(y\)</span> as possible. Figure <a href="linear-projection.html#fig:lp-demo1">3.3</a> depicts this set up visually – each dimension is an observation, each column in the matrix is represented a vector, and the column space of <span class="math inline">\((X,c)\)</span> is the shaded grey plane. The vector <span class="math inline">\(y\)</span> lies off this plane.</p>
<div class="figure"><span id="fig:lp-demo1"></span>
<img src="images/lp_solve1.png" alt="Schematic of orthogonal projection as a geometric problem" width="100%" />
<p class="caption">
Figure 3.3: Schematic of orthogonal projection as a geometric problem
</p>
</div>
<p>From our discussion in <a href="linear-projection.html#proj_def">Section 3.1</a>, we know that the “best” vector is the orthogonal projection from the column space to the vector <span class="math inline">\(y\)</span>. This is the shortest possible distance between the flat plane and the observed outcome, and is just <span class="math inline">\(\hat{y}\)</span>. Moreover, since <span class="math inline">\(\hat{y}\)</span> lies on the column space, we know we only need to combine some scaled amount of <span class="math inline">\(X\)</span> and <span class="math inline">\(c\)</span> to define the vector <span class="math inline">\(\hat{y}\)</span>, i.e., <span class="math inline">\(\beta_1X + \beta_0c\)</span>. Figure <a href="linear-projection.html#fig:lp-demo2">3.4</a> shows this geometrically. And in fact, the scalar coefficients <span class="math inline">\(\beta_1, \beta_0\)</span> in this case are just the regression coefficients derived from OLS. Why? Because we know that the orthogonal projection of <span class="math inline">\(y\)</span> onto the column space minimises the error between our prediction <span class="math inline">\(\hat{y}\)</span> and the observed outcome vector <span class="math inline">\(y\)</span>. This is the same as the minimisation problem that OLS solves, as outlined at the beginning of this section!</p>
<div class="figure"><span id="fig:lp-demo2"></span>
<img src="images/lp_solve2.png" alt="Relation of orthogonal projection to linear regression." width="100%" />
<p class="caption">
Figure 3.4: Relation of orthogonal projection to linear regression.
</p>
</div>
<p>Consider any other vector on the column space, and the distance between itself and and <span class="math inline">\(y\)</span>. Each non-orthogonal vector would be longer, and hence have a larger predictive error, than <span class="math inline">\(\hat{y}\)</span>. For example, Figure <a href="linear-projection.html#fig:lp-distances">3.5</a> plots two alternative vectors on <span class="math inline">\(col(X,c)\)</span> alongside <span class="math inline">\(\hat{y}\)</span>. Clearly, <span class="math inline">\(\hat{\epsilon} &lt; \epsilon&#39; &lt; \epsilon&#39;&#39;,\)</span> and this is true of any other vector on the column space too.</p>
<div class="figure"><span id="fig:lp-distances"></span>
<img src="images/lp_distances.png" alt="Alternative vectors on the column space are further away from y." width="100%" />
<p class="caption">
Figure 3.5: Alternative vectors on the column space are further away from y.
</p>
</div>
<p>Hence, linear projection and linear regression can be seen (both algebraically and geometrically) to be solving the same problem – minimising the (squared) distance between an observed vector <span class="math inline">\(y\)</span> and prediction vector <span class="math inline">\(\hat{y}\)</span>. This demonstration generalises to many dimensions (observations), though of course it becomes much harder to intuit the geometry of highly-dimensional data. And similarly, with more observations we could also extend the number of predictors too such that <span class="math inline">\(X\)</span> is not a single column vector but a matrix of predictor variables (i.e. multivariate regression). Again, visualising what the column space of this matrix would look like geometrically becomes harder.</p>
<p>To summarise, this section has demonstrated two features. First, that linear regression simply is an orthogonal projection. We saw this algebraically by noting that the derivation of OLS coefficients, and subsequently the predicted values from a linear regression, is identical to <span class="math inline">\(Py\)</span> (where <span class="math inline">\(P\)</span> is a projection matrix). Second, and geometrically, we intuited why this is the case: namely that projecting onto a lower-dimensional column space involves finding the linear combination of predictors that minimises the Euclidean distance to <span class="math inline">\(y\)</span>, i.e. <span class="math inline">\(\hat{y}\)</span>. The scalars we use to do so are simply the regression coefficients we would generate using OLS regression.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>Since <span class="math inline">\(P\)</span> is (in the finite case) a square matrix, a projection matrix is an idempotent matrix – I discuss this property in more detail later on in this note.<a href="linear-projection.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>This final section borrows heavily from <a href="https://www.youtube.com/watch?v=My51wdv2Uz0">Ben Lambert’s explanation of projection</a> and a <a href="http://andy.egge.rs/teaching/ols_projection.html">demonstration using R by Andy Eggers</a>.<a href="linear-projection.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exp-ineq.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="wlln.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-linear-projection.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["10EconometricTheorems.pdf", "10EconometricTheorems.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
